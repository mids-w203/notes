# Ordinary Least Squares
Let $Y$ be our outcome random variable and 
$$
\pmb{X}=\begin{bmatrix}
1 \\ X_1 \\ X_2 \\ \vdots \\ X_k
\end{bmatrix}_{(k+1)\times 1}
$$ 
be our predictor vector containing $k$ predictors and a constant. We denote the joint distribution of $(Y,\pmb{X})$ by $F(y,\pmb{x})$, i.e.,
$$
F(y,\pmb{x})=\mathbb{P}(Y\leq y, \pmb{X}\leq\pmb{x})
=\mathbb{P}(Y\leq y,X_1\leq x_1,\ldots,X_k\leq x_k).
$$

The **dataset** or **sample** is a collection of observations $\{(Y_i,\pmb{X}_i): i=1,2,\ldots,n\}$. We assume that each observation
$(Y_i,\pmb{X}_i)$ is a random vector drawn from the common distribution or **population** $F$.   

## Matrix Formulation
For a given vector of (unknown) coefficients 
$\pmb{\beta}=\begin{bmatrix}\beta_0 & \beta_1 & \ldots & \beta_k\end{bmatrix}'\in\mathbb{R}^{k+1}$, we define the following **cost function**:
$$
\widehat{S}(\pmb{\beta})=\frac{1}{n}\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2.
$$
The cost function $\widehat{S}({\pmb{\beta}})$ can also be thought of as the average sum of residuals. In fact, $\widehat{S}({\pmb{\beta}})$ is the moment (plug-in) estimator of the mean squared error, 
$$
S(\pmb{\beta})=\E{(Y-\pmb{X}'\pmb{\beta})^2}.
$$

We now minimize $\widehat{S}({\pmb{\beta}})$ over all possible choices of
$\pmb{\beta}\in\mathbb{R}^{k+1}$. When the minimizer exists and is unique, we call it the **least squares estimator**, denoted $\widehat{\pmb{\beta}}$. 

:::{.definition name="(Ordinary) Least Squares Estimator"}
The least square estimator is $$
\widehat{\pmb{\beta}}
=\underset{\pmb{\beta}\in\mathbb{R}^{k+1}}{\arg\min} \ \widehat{S}(\pmb{\beta}),
$$
provided it exists uniquely.
:::

## Solution of OLS
We rewrite the cost function as
$$
\widehat{S}(\pmb{\beta})=\frac{1}{n}SSE(\pmb{\beta}),
$$
where 
$SSE(\pmb{\beta}):=\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2$.

We now express $SSE(\pmb{\beta})$ as a quadratic function of $\pmb{\beta}'$.
$$
\begin{align}
SSE &=\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i(\pmb{X_i}'\pmb{\beta})
  + \sum\limits_{i=1}^n (\pmb{X_i}'\pmb{\beta})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i(\pmb{\beta}'\pmb{X_i})
  + \sum\limits_{i=1}^n (\pmb{X_i}'\pmb{\beta})(\pmb{X_i}'\pmb{\beta}) \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n \pmb{\beta}'(Y_i\pmb{X_i})
  + \sum\limits_{i=1}^n (\pmb{\beta}'\pmb{X_i})(\pmb{X_i}'\pmb{\beta}) \\
&=\left(\sum\limits_{i=1}^n Y_i^2\right) 
  - 2\pmb{\beta}'\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right)
  + \pmb{\beta}'\left(\sum\limits_{i=1}^n \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}
\end{align}
$$
Taking partial derivative w.r.t. $\beta_j$, we get
$$
\frac{\partial}{\partial\beta_j}SSE(\pmb{\beta})=-2\left[\sum\limits_{i=1}^n\pmb{X_i}Y_i\right]_j 
+ 2\left[\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}\right]_j.
$$

Therefore,
$$
\frac{\partial}{\partial\pmb{\beta}}SSE(\pmb{\beta})
=-2\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right) 
+ 2\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}.
$$

In order to miniminize $SSE(\pmb{\beta})$, a necessary condition for $\widehat{\pmb{\beta}}$ is
$$
\frac{\partial}{\partial\pmb{\beta}}SSE(\pmb{\beta})\bigg|_{\pmb{\beta}
=\widehat{\pmb{\beta}}}=\pmb{0},
$$
i.e.,
$$
-2\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right) 
+ 2\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\widehat{\pmb{\beta}}
=\pmb{0}
$$
So,
\begin{equation}
\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right)
=\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\widehat{\pmb{\beta}}
(\#eq:moment-0)
\end{equation}

Both the left and right hand side of the above equation are $k+1$ vectors. So, we have a system of $(k+1)$ linear equations with $(k+1)$ unknowns---the elements of $\pmb{\beta}$. 

Let us define

$$
\widehat{\mathbb{Q}}_{\pmb{XX}}
=\frac{1}{n}\left(\sum\limits_{i=1}^n\pmb{X_i}\pmb{X_i}'\right)
\mbox{ and }
\widehat{\mathbb{Q}}_{\pmb{X}Y}
=\frac{1}{n}\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right).
$$

Rewriting \@ref(eq:moment-0), we get
$$
\begin{equation}
\widehat{\mathbb{Q}}_{\pmb{X}Y}=\widehat{\mathbb{Q}}_{\pmb{XX}}
\widehat{\pmb{\beta}}.
(\#eq:moment)
\end{equation}
$$
Equation \@ref(eq:moment) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that
$\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular. In that case, we can solve for $\widehat{\pmb{\beta}}$ to get,
$$
\widehat{\pmb{\beta}}=\left[\widehat{\mathbb{Q}}_{\pmb{XX}}\right]^{-1}
\widehat{\mathbb{Q}}_{\pmb{X}Y}.
$$
To verify that the above choice minimizes $SSE(\pmb{\beta})$,
one can consider the second-order moment conditions.
$$
\frac{\partial^2}{\partial\pmb{\beta}\partial\pmb{\beta}'}SSE(\pmb{\beta})
=2\widehat{\mathbb{Q}}_{\pmb{XX}}.
$$
If $\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular, it is also positive-definite. So, we have actually proved the following theorem.

:::{.theorem}
If \ $\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular, then the
least squares estimator is unique, and is given by
$$
\widehat{\pmb{\beta}}=\left[\widehat{\mathbb{Q}}_{\pmb{XX}}\right]^{-1}
\widehat{\mathbb{Q}}_{\pmb{X}Y}.
$$
:::


## Errors and Residuals
We first define the **fitted** value as
$$
\widehat{Y}_i=\pmb{X}_i'\widehat{\pmb{\beta}}\mbox{ for }
i=1,2,\ldots,n.
$$
For the least squares estimators, we define the **errors** and **residuals** in the following way:
$$
e_i=Y_i-\pmb{X}'\pmb{\beta}, \mbox{ and } 
\widehat{e}_i=Y_i-\widehat{Y}_i.
$$

:::{.theorem #olserror name="Least Squares Error"}
If \ $\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular, then  
1. $\sum\limits_{i=1}^n\pmb{X}_i\widehat{e}_i=\pmb{0}$  
2. $\sum\limits_{i=1}^n\widehat{e}_i=0$
:::

:::{.proof}
\begin{align}
\sum\limits_{i=1}^n\pmb{X}_i\widehat{e}_i 
&=\sum\limits_{i=1}^n\pmb{X}_i(Y_i-\widehat{Y}_i) \\
&=\sum\limits_{i=1}^n\pmb{X}_iY_i-\sum\limits_{i=1}^n\pmb{X}_i\widehat{Y}_i \\
&=\sum\limits_{i=1}^n\pmb{X}_iY_i-\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'\pmb{\widehat{\beta}} \\
&=\widehat{Q}_{\pmb{X}Y}-\widehat{Q}_{\pmb{XX}}\pmb{\widehat{\beta}} \\
&=\widehat{Q}_{\pmb{X}Y}-\widehat{Q}_{\pmb{XX}}
\left( \widehat{Q}_{\pmb{XX}}^{-1} \widehat{Q}_{\pmb{X}Y} \right) \\
&=\pmb{0}
\end{align}

From the first row of (1) we get

$$
\sum\limits_{i=1}^n X_{i1}\widehat{e}_i=0.
$$
Since $X_{i1}=1$ for all $i$, we have that
$$
\sum\limits_{i=1}^n\widehat{e}_i=0.
$$
Hence the result.
:::


## Model in Matrix Notation
Taking the definition of errors from the last section, we can write down a system of $n$ linear equations:
\begin{align}
Y_1 &= \pmb{X_1}'\pmb{\beta} + e_1 \\
Y_2 &= \pmb{X_2}'\pmb{\beta} + e_2 \\
& \vdots \\
Y_n &= \pmb{X_1}'\pmb{\beta} + e_n
\end{align}

Define 
$$
\pmb{Y}=\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}_{n\times1},\ 
\mathbb{X}=\begin{bmatrix}
\pmb{X}_1 \\
\pmb{X}_2 \\
\vdots \\
\pmb{X}_n
\end{bmatrix}_{n\times(k+1)}, \mbox{ and }
\pmb{e}=\begin{bmatrix}
e_1 \\
e_2 \\
\vdots \\
e_n
\end{bmatrix}_{n\times1}.
$$
We can now rewrite the system as the following:
$$
\pmb{Y}=\mathbb{X}\pmb{\beta}+\pmb{e}.
$$
Note that
$$
\mathbb{X}=\begin{bmatrix}
1 & X_{11} & X_{12} & \ldots & X_{1k} \\
1 & X_{21} & X_{22} & \ldots & X_{2k} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{n1} & X_{n2} & \ldots & X_{nk}
\end{bmatrix}
$$

We also note that
$$
\widehat{Q}_{\pmb{XX}}=\sum\limits_{i=1}^n\pmb{X}_i'\pmb{X}_i=
\mathbb{X}'\mathbb{X},
$$
and
$$
\widehat{Q}_{\pmb{X}Y}=\sum\limits_{i=1}^n\pmb{X}_iY_i=
\mathbb{X}'\pmb{Y}.
$$
So, we have write the least squares estimator as
$$
\widehat{\pmb{\beta}}=\left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}\pmb{Y}.
$$
Similarly, the residual vector is
$$
\widehat{\pmb{e}}=\pmb{Y}-\mathbb{X}\widehat{\pmb{\beta}}.
$$
As a consequence, we can write
$$
\mathbb{X}'\widehat{\pmb{e}}=\pmb{0}.
$$
