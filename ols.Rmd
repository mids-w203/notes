# Ordinary Least Squares
Let $Y$ be our outcome random variable and 
$$
\pmb{X}=\begin{bmatrix}
1 \\ X_1 \\ X_2 \\ \vdots \\ X_k
\end{bmatrix}_{(k+1)\times 1}
$$ 
be our predictor vector containing $k$ predictors and a constant. We denote the joint distribution of $(Y,\pmb{X})$ by $F(y,\pmb{x})$, i.e.,
$$
F(y,\pmb{x})=\mathbb{P}(Y\leq y, \pmb{X}\leq\pmb{x})
=\mathbb{P}(Y\leq y,X_1\leq x_1,\ldots,X_k\leq x_k).
$$

The **dataset** or **sample** is a collection of observations $\{(Y_i,\pmb{X}_i): i=1,2,\ldots,n\}$. We assume that each observation
$(Y_i,\pmb{X}_i)$ is a random vector drawn from the common distribution or **population** $F$.   

## Matrix Formulation
For a given vector of (unknown) coefficients 
$\pmb{\beta}=\begin{bmatrix}\beta_0 & \beta_1 & \ldots & \beta_k\end{bmatrix}'\in\mathbb{R}^{k+1}$, we define the following **cost function**:
$$
\widehat{S}(\pmb{\beta})=\frac{1}{n}\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2.
$$
The cost function $\widehat{S}({\pmb{\beta}})$ can also be thought of as the average sum of residuals. In fact, $\widehat{S}({\pmb{\beta}})$ is the moment (plug-in) estimator of the mean squared error, 
$$
S(\pmb{\beta})=\E{(Y-\pmb{X}'\pmb{\beta})^2}.
$$

We now minimize $\widehat{S}({\pmb{\beta}})$ over all possible choices of
$\pmb{\beta}\in\mathbb{R}^{k+1}$. When the minimizer exists and is unique, we call it the **least squares estimator**, denoted $\widehat{\pmb{\beta}}$. 

:::{.definition name="(Ordinary) Least Squares Estimator"}
The least square estimator is $$
\widehat{\pmb{\beta}}
=\underset{\pmb{\beta}\in\mathbb{R}^{k+1}}{\arg\min} \ \widehat{S}(\pmb{\beta}),
$$
provided it exists uniquely.
:::

## Solution of OLS
We rewrite the cost function as
$$
\widehat{S}(\pmb{\beta})=\frac{1}{n}SSE(\pmb{\beta}),
$$
where 
$SSE(\pmb{\beta}):=\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2$.

We now express $SSE(\pmb{\beta})$ as a quadratic function of $\pmb{\beta}'$.
$$
\begin{align}
SSE &=\sum\limits_{i=1}^n(Y_i-\pmb{X_i}'\pmb{\beta})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i(\pmb{X_i}'\pmb{\beta})
  + \sum\limits_{i=1}^n (\pmb{X_i}'\pmb{\beta})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i(\pmb{\beta}'\pmb{X_i})
  + \sum\limits_{i=1}^n (\pmb{X_i}'\pmb{\beta})(\pmb{X_i}'\pmb{\beta}) \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n \pmb{\beta}'(Y_i\pmb{X_i})
  + \sum\limits_{i=1}^n (\pmb{\beta}'\pmb{X_i})(\pmb{X_i}'\pmb{\beta}) \\
&=\left(\sum\limits_{i=1}^n Y_i^2\right) 
  - 2\pmb{\beta}'\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right)
  + \pmb{\beta}'\left(\sum\limits_{i=1}^n \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}
\end{align}
$$
Taking partial derivative w.r.t. $\beta_j$, we get
$$
\frac{\partial}{\partial\beta_j}SSE(\pmb{\beta})=-2\left[\sum\limits_{i=1}^n\pmb{X_i}Y_i\right]_j 
+ 2\left[\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}\right]_j.
$$

Therefore,
$$
\frac{\partial}{\partial\pmb{\beta}}SSE(\pmb{\beta})
=-2\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right) 
+ 2\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\pmb{\beta}.
$$

In order to miniminize $SSE(\pmb{\beta})$, a necessary condition for $\widehat{\pmb{\beta}}$ is
$$
\frac{\partial}{\partial\pmb{\beta}}SSE(\pmb{\beta})\bigg|_{\pmb{\beta}
=\widehat{\pmb{\beta}}}=\pmb{0},
$$
i.e.,
$$
-2\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right) 
+ 2\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\widehat{\pmb{\beta}}
=\pmb{0}
$$
So,
\begin{equation}
\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right)
=\left(\sum\limits_{i=1}^n  \pmb{X_i}\pmb{X_i}'\right)\widehat{\pmb{\beta}}
(\#eq:moment-0)
\end{equation}

Both the left and right hand side of the above equation are $k+1$ vectors. So, we have a system of $(k+1)$ linear equations with $(k+1)$ unknowns---the elements of $\pmb{\beta}$. 

Let us define

$$
\widehat{\mathbb{Q}}_{\pmb{XX}}
=\frac{1}{n}\left(\sum\limits_{i=1}^n\pmb{X_i}\pmb{X_i}'\right)
\mbox{ and }
\widehat{\mathbb{Q}}_{\pmb{X}Y}
=\frac{1}{n}\left(\sum\limits_{i=1}^n\pmb{X_i}Y_i\right).
$$

Rewriting \@ref(eq:moment-0), we get
$$
\begin{equation}
\widehat{\mathbb{Q}}_{\pmb{X}Y}=\widehat{\mathbb{Q}}_{\pmb{XX}}
\widehat{\pmb{\beta}}.
(\#eq:moment)
\end{equation}
$$
Equation \@ref(eq:moment) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that
$\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular. In that case, we can solve for $\widehat{\pmb{\beta}}$ to get,
$$
\widehat{\pmb{\beta}}=\left[\widehat{\mathbb{Q}}_{\pmb{XX}}\right]^{-1}
\widehat{\mathbb{Q}}_{\pmb{X}Y}.
$$
To verify that the above choice minimizes $SSE(\pmb{\beta})$,
one can consider the second-order moment conditions.
$$
\frac{\partial^2}{\partial\pmb{\beta}\partial\pmb{\beta}'}SSE(\pmb{\beta})
=2\widehat{\mathbb{Q}}_{\pmb{XX}}.
$$
If $\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular, it is also positive-definite. So, we have actually proved the following theorem.

:::{.theorem}
If \ $\widehat{\mathbb{Q}}_{\pmb{XX}}$ is non-singular, then the
least squares estimator is unique, and is given by
$$
\widehat{\pmb{\beta}}=\left[\widehat{\mathbb{Q}}_{\pmb{XX}}\right]^{-1}
\widehat{\mathbb{Q}}_{\pmb{X}Y}.
$$
:::


## Least Squares Residuals
