# Large-Sample Regression
We assume that the best linear predictor, $\mathscr{P}[Y|\pmb{X}]$, of $Y$ given $\pmb{X}$ is $\pmb{X}'\pmb{\beta}$. If we write
$$
Y=\pmb{X}'\pmb{\beta}+e.
$$
we have from Theorem \@ref(thm:blperror)
$$\E{e}=0,\mbox{ and }\E{\pmb{X}e}=\pmb{0}.$$

We also assume that the dataset $\{(Y_i,\pmb{X}_i)\}$ are taken **i.i.d.** from the joint distribution of $(Y,\pmb{X})$. For each $i$, we can write
$$
Y_i=\pmb{X_i}'\pmb{\beta}+e_i.
$$
In matrix notation, we can write
$$
\pmb{Y}=\mathbb{X}'\pmb{\beta}+\pmb{e}.
$$
Then
$$\E{\pmb{e}}=\pmb{0}$$.

## Consistency of OLS Estimators

## Asymptotic Normality
We start by revealing an alternative expression for the OLS estimators $\widehat{\pmb{\beta}}$ using matrix notation.
\begin{align}
\widehat{\pmb{\beta}}
&=\left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}'\pmb{Y} \\
&=\left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}'(\mathbb{X}\pmb{\beta}+\pmb{e}) \\
&=\left[\mathbb{X}'\mathbb{X}\right]^{-1}(\mathbb{X}'\mathbb{X})\pmb{\beta}+
\left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}'\pmb{e} \\
&=\pmb{\beta} + \left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}'\pmb{e}
\end{align}
So,
\begin{equation}
\widehat{\pmb{\beta}}-\pmb{\beta} = \left[\mathbb{X}'\mathbb{X}\right]^{-1}\mathbb{X}'\pmb{e}
(\#eq:beta)
\end{equation}

We can then multiply by $\sqrt{n}$ both sides of Equation \@ref(eq:beta) to get
\begin{align}
\sqrt{n}\left(\widehat{\pmb{\beta}}-\pmb{\beta}\right)
&=\left( \frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i' \right)^{-1}
\left( \frac{1}{\sqrt{n}}\sum\limits_{i=1}^n\pmb{X}_ie_i \right) \\
&=\widehat{\mathbb Q}_{\pmb{XX}}^{-1}
\left( \frac{1}{\sqrt{n}}\sum\limits_{i=1}^n\pmb{X}_ie_i \right)
\end{align}
From the consistency of OLS estimators, we already have 
$$ \widehat{\mathbb Q}_{\pmb{XX}}\xrightarrow[p]{\quad\quad}\mathbb{Q}_{\pmb{XX}}$$
Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression.

We first note (from i.i.d. and Theorem \@ref(thm:blperror)) that 
$$
\E{\pmb{X}_ie_i}=\E{\pmb{X}e}=\pmb{0}.
$$
Let us compute the covariance matrix of $\pmb{X}_ie_i$. Since the expectation vector is zero, we have 
$$
\mathbb{V}[\pmb{X}_ie_i]=\E{\pmb{X}_ie_i(\pmb{X}_ie_i)'}=\E{\pmb{X}e(\pmb{X}e)'}=\E{\pmb{X}\pmb{X}'e^2}\stackrel{\text{def}}{=}\mathbb{A}.
$$
As any function of $\{(Y_i,\pmb{X}_i)\}$'s are independent,  $\{\pmb{X}_ie_i\}$'s are independent. By the (multivariate) **Central Limit Theorem**, as $n\to\infty$
$$
\frac{1}{\sqrt{n}}\sum\limits_{i=1}^n\pmb{X}_ie_i
\xrightarrow[d]{\quad\quad}\mathcal{N}(\pmb{0},\mathbb{A}).
$$
There is a small technicality here, we must have $\mathbb{A}<\infty$. This can be imposed by a stronger regularity condition on the moments, e.g.,
$\E{Y^4},\E{||\pmb{X}||^4}<\infty$.
Putting everything together, we conclude
$$
\sqrt{n}(\widehat{\pmb{\beta}}-\pmb{\beta})\xrightarrow[d]{\quad\quad}
\mathbb{Q}_{\pmb{XX}}^{-1}\mathcal{N}(\pmb{0},\mathbb{A})
=\mathcal{N}\left(0,\mathbb{Q}_{\pmb{XX}}^{-1}\mathbb{A}\mathbb{Q}_{\pmb{XX}}^{-1}\right)
$$

:::{.theorem #asympvar name="Asymptotic Distribution of OLS Estimators"}
We assume the following:  
1. The observations $\{(Y_i,\pmb{X}_i)\}_{i=1}^n$ are i.i.d from the joint
distribution of $(Y,\pmb{X})$  
2. $\E{Y^4}<\infty$  
3. $\E{||\pmb{X}||^4}<\infty$  
4. $\mathbb{Q}_{\pmb{XX}}=\E{\pmb{X}\pmb{X}'}$ is positive-definite.
Under these assumptions, as $n\to\infty$
$$
\sqrt{n}(\widehat{\pmb{\beta}}-\pmb{\beta})\xrightarrow[d]{\quad\quad}
\mathcal{N}\left(\pmb{0},\mathbb{V}_{\pmb{\beta}}\right),
$$
where 
$$\mathbb{V}_{\pmb{\beta}}\stackrel{\text{def}}{=}\mathbb{Q}_{\pmb{XX}}^{-1}\mathbb{A}\mathbb{Q}_{\pmb{XX}}^{-1}$$
and $\mathbb{Q}_{\pmb{XX}}=\E{\pmb{X}\pmb{X}'}$, $\mathbb{A}=\E{\pmb{X}\pmb{X}'e^2}$.
:::
The covariance matrix $\mathbb{V}_{\pmb{\beta}}$ is called the **asymptotic variance matrix** of $\widehat{\pmb{\beta}}$. The matrix is sometimes referred to as the **sandwich** form.

## Covariance Matrix Estimation
We now turn our attention to the estimation of the sandwich matrix using a finite sample.

### Heteroskedastic Variance
Theorem \@ref(thm:asympvar) presented the asymptotic covariance matrix of
$\sqrt{n}(\widehat{\pmb{\beta}}-\pmb{\beta})$ is
$$\mathbb{V}_{\pmb{\beta}}
=\mathbb{Q}_{\pmb{XX}}^{-1}\mathbb{A}\mathbb{Q}_{\pmb{XX}}^{-1}.$$
Without imposing any homoskedasticity condition, we estimate $\mathbb{V}_{\pmb{\beta}}$ using a plug-in estimator.

We have already seen that $\widehat{\mathbb{Q}}_{\pmb{XX}}=\frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'$ is a natural estimator for $\mathbb{Q}_{\pmb{XX}}$. For $\mathbb{A}$, we use the moment estimator
$$
\widehat{\mathbb{A}}=\frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'\widehat{e}_i^2,
$$
where $\widehat{e}_i=(Y_i-\pmb{X}_i'\widehat{\pmb{\beta}})$ is the $i$-th residual. As it turns out, $\widehat{\mathbb{A}}$ is a consistent estimator
for $\mathbb{A}$.


As a result, we get the following plug-in estimator for $\mathbb{V}_{\pmb{\beta}}$:
$$
\widehat{\mathbb{V}}_{\pmb{\beta}}^{\text{HC0}}=
\widehat{\mathbb{Q}}_{\pmb{XX}}^{-1}\widehat{\mathbb{A}}\widehat{\mathbb{Q}}_{\pmb{XX}}^{-1}
$$
The estimator is also consistent. For a proof, see Hensen 2013.

As a consequence, we can get the following estimator for the variance, $\mathbb{V}_{\widehat{\pmb{\beta}}}$, of $\widehat{\pmb{\beta}}$ in the heteroskedastic case.
\begin{align}
\widehat{\mathbb{V}}^{\text{HC0}}_{\widehat{\pmb{\beta}}}
&=\frac{1}{n}\widehat{\mathbb{V}}_{\pmb{\beta}}^{\text{HC0}} \\
&=\frac{1}{n}\widehat{\mathbb{Q}}_{\pmb{XX}}^{-1}\widehat{\mathbb{A}}\widehat{\mathbb{Q}}_{\pmb{XX}}^{-1} \\
&=\frac{1}{n}\left(\frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'\right)^{-1}
\left(\frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'\widehat{e}_i^2\right)
\left(\frac{1}{n}\sum\limits_{i=1}^n\pmb{X}_i\pmb{X}_i'\right)^{-1} \\
&=\left(\mathbb{X}\mathbb{X}'\right)^{-1}
\mathbb{X}\mathbb{D}\mathbb{X}'
\left(\mathbb{X}\mathbb{X}'\right)^{-1}
\end{align}
where $\mathbb{D}$ is an $n\times n$ diagonal matrix with diagonal entries $\widehat{e}_1^2,\widehat{e}_2^2,\ldots,\widehat{e}_n^2$.
The estimator $\widehat{\mathbb{V}}^{\text{HC0}}_{\widehat{\pmb{\beta}}}$ is referred to as the **robust error variance estimator** for the OLS coefficients $\widehat{\pmb{\beta}}$.

### Homeskedastic Variance
