[["large-sample-regression.html", "Chapter 4 Large-Sample Regression", " Chapter 4 Large-Sample Regression We assume that the best linear predictor, \\(\\mathscr{P}[Y|\\pmb{X}]\\), of \\(Y\\) given \\(\\pmb{X}\\) is \\(\\pmb{X}&#39;\\pmb{\\beta}\\). If we write \\[ Y=\\pmb{X}&#39;\\pmb{\\beta}+e. \\] we have from Theorem ?? \\[{\\mathbb{E}\\left[ e \\right]}=0,\\mbox{ and }{\\mathbb{E}\\left[ \\pmb{X}e \\right]}=\\pmb{0}.\\] We also assume that the dataset \\(\\{(Y_i,\\pmb{X}_i)\\}\\) are taken i.i.d. from the joint distribution of \\((Y,\\pmb{X})\\). For each \\(i\\), we can write \\[ Y_i=\\pmb{X_i}&#39;\\pmb{\\beta}+e_i. \\] In matrix notation, we can write \\[ \\pmb{Y}=\\mathbb{X}&#39;\\pmb{\\beta}+\\pmb{e}. \\] Then \\[{\\mathbb{E}\\left[ \\pmb{e} \\right]}=\\pmb{0}\\]. "],["consistency-of-ols-estimators.html", "4.1 Consistency of OLS Estimators", " 4.1 Consistency of OLS Estimators "],["asymptotic-normality.html", "4.2 Asymptotic Normality", " 4.2 Asymptotic Normality We start by revealing an alternative expression for the OLS estimators \\(\\widehat{\\pmb{\\beta}}\\) using matrix notation. \\[\\begin{align} \\widehat{\\pmb{\\beta}} &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{Y} \\\\ &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;(\\mathbb{X}\\pmb{\\beta}+\\pmb{e}) \\\\ &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}(\\mathbb{X}&#39;\\mathbb{X})\\pmb{\\beta}+ \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\\\ &amp;=\\pmb{\\beta} + \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\end{align}\\] So, \\[\\begin{equation} \\widehat{\\pmb{\\beta}}-\\pmb{\\beta} = \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\tag{4.1} \\end{equation}\\] We can then multiply by \\(\\sqrt{n}\\) both sides of Equation (4.1) to get \\[\\begin{align} \\sqrt{n}\\left(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta}\\right) &amp;=\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39; \\right)^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\right) \\\\ &amp;=\\widehat{\\mathbb Q}_{\\pmb{XX}}^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\right) \\end{align}\\] From the consistency of OLS estimators, we already have \\[ \\widehat{\\mathbb Q}_{\\pmb{XX}}\\xrightarrow[p]{\\quad\\quad}\\mathbb{Q}_{\\pmb{XX}}\\] Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression. We first note (from i.i.d. and Theorem ??) that \\[ {\\mathbb{E}\\left[ \\pmb{X}_ie_i \\right]}={\\mathbb{E}\\left[ \\pmb{X}e \\right]}=\\pmb{0}. \\] Let us compute the covariance matrix of \\(\\pmb{X}_ie_i\\). Since the expectation vector is zero, we have \\[ \\mathbb{V}[\\pmb{X}_ie_i]={\\mathbb{E}\\left[ \\pmb{X}_ie_i(\\pmb{X}_ie_i)&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{X}e(\\pmb{X}e)&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39;e^2 \\right]}\\stackrel{\\text{def}}{=}\\mathbb{A}. \\] As any function of \\(\\{(Y_i,\\pmb{X}_i)\\}\\)’s are independent, \\(\\{\\pmb{X}_ie_i\\}\\)’s are independent. By the (multivariate) Central Limit Theorem, as \\(n\\to\\infty\\) \\[ \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\xrightarrow[d]{\\quad\\quad}\\mathcal{N}(\\pmb{0},\\mathbb{A}). \\] There is a small technicality here, we must have \\(\\mathbb{A}&lt;\\infty\\). This can be imposed by a stronger regularity condition on the moments, e.g., \\({\\mathbb{E}\\left[ Y^4 \\right]},{\\mathbb{E}\\left[ ||\\pmb{X}||^4 \\right]}&lt;\\infty\\). Putting everything together, we conclude \\[ \\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\xrightarrow[d]{\\quad\\quad} \\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathcal{N}(\\pmb{0},\\mathbb{A}) =\\mathcal{N}\\left(0,\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\right) \\] Theorem 4.1 (Asymptotic Distribution of OLS Estimators) We assume the following: 1. The observations \\(\\{(Y_i,\\pmb{X}_i)\\}_{i=1}^n\\) are i.i.d from the joint distribution of \\((Y,\\pmb{X})\\) 2. \\({\\mathbb{E}\\left[ Y^4 \\right]}&lt;\\infty\\) 3. \\({\\mathbb{E}\\left[ ||\\pmb{X}||^4 \\right]}&lt;\\infty\\) 4. \\(\\mathbb{Q}_{\\pmb{XX}}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39; \\right]}\\) is positive-definite. Under these assumptions, as \\(n\\to\\infty\\) \\[ \\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\xrightarrow[d]{\\quad\\quad} \\mathcal{N}\\left(\\pmb{0},\\mathbb{V}_{\\pmb{\\beta}}\\right), \\] where \\[\\mathbb{V}_{\\pmb{\\beta}}\\stackrel{\\text{def}}{=}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\] and \\(\\mathbb{Q}_{\\pmb{XX}}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39; \\right]}\\), \\(\\mathbb{A}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39;e^2 \\right]}\\). The covariance matrix \\(\\mathbb{V}_{\\pmb{\\beta}}\\) is called the asymptotic variance matrix of \\(\\widehat{\\pmb{\\beta}}\\). The matrix is sometimes referred to as the sandwich form. "],["covariance-matrix-estimation.html", "4.3 Covariance Matrix Estimation", " 4.3 Covariance Matrix Estimation We now turn our attention to the estimation of the sandwich matrix using a finite sample. 4.3.1 Heteroskedastic Variance Theorem 4.1 presented the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\) is \\[\\mathbb{V}_{\\pmb{\\beta}} =\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}.\\] Without imposing any homoskedasticity condition, we estimate \\(\\mathbb{V}_{\\pmb{\\beta}}\\) using a plug-in estimator. We have already seen that \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}=\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\) is a natural estimator for \\(\\mathbb{Q}_{\\pmb{XX}}\\). For \\(\\mathbb{A}\\), we use the moment estimator \\[ \\widehat{\\mathbb{A}}=\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\widehat{e}_i^2, \\] where \\(\\widehat{e}_i=(Y_i-\\pmb{X}_i&#39;\\widehat{\\pmb{\\beta}})\\) is the \\(i\\)-th residual. As a result, we get the following plug-in estimator for \\(\\mathbb{V}_{\\pmb{\\beta}}\\): \\[ \\widehat{\\mathbb{V}}_{\\pmb{\\beta}}^{\\text{HC0}}= \\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1}\\widehat{\\mathbb{A}}\\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1} \\] 4.3.2 Homeskedastic Variance "]]
