[["index.html", "w203: Statistics for Data Science Cover", " w203: Statistics for Data Science w203 Instructors 2022-02-05 Cover "],["regression.html", "Chapter 1 Regression", " Chapter 1 Regression We write a \\(k\\)-vector (of scalars) as \\[ \\pmb{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} \\] The transpose of \\(\\pmb{x}\\) as \\[ \\pmb{x}&#39;= \\begin{bmatrix} x_1 &amp; x_2 &amp; \\ldots &amp; x_k \\end{bmatrix}. \\] We use uppercase letters \\(X,Y,Z,\\ldots\\) to denote random variables. Random vectors are denoted by bold uppercase letters \\(\\pmb{X},\\pmb{Y},\\pmb{Z},\\ldots\\), and written as a column vector. For example, \\[ \\pmb{X}= \\begin{bmatrix} X_{1}\\\\ X_{2}\\\\ \\vdots \\\\ X_{k} \\\\ \\end{bmatrix}_{k\\times 1} \\] In order to distinguish random matrices from vectors, a random matrix is denoted by \\(\\mathbb{X}\\). The expectation of \\(\\pmb{X}\\) is defined as \\[ \\mathbb{E}[\\pmb{X}]= \\begin{bmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_k] \\\\ \\end{bmatrix} \\] The \\(k\\times k\\) covariance matrix of \\(\\pmb{X}\\) is \\[ \\begin{align} V[\\pmb{X}] &amp;=\\mathbb{E}[(\\pmb{X}-\\mathbb{E}[\\pmb{X}])(\\pmb{X}-\\mathbb{E}[\\pmb{X}])&#39;] \\\\ &amp;=\\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\ldots &amp; \\sigma_{1k} \\\\ \\sigma_{21} &amp; \\sigma_{2}^2 &amp; \\ldots &amp; \\sigma_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k1} &amp; \\sigma_{k2}^2 &amp; \\ldots &amp; \\sigma_{k}^2 \\\\ \\end{bmatrix}_{k\\times k} \\end{align} \\] where \\(\\sigma_j=V[X_j]\\) and \\(\\sigma_{ij}=Cov[X_i,X_j]\\) for \\(i,j=1,2,\\ldots,k\\) and \\(i\\neq j\\). "],["conditional-expectation-function.html", "1.1 Conditional Expectation Function", " 1.1 Conditional Expectation Function Theorem 1.1 If  \\({\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty\\) and \\(\\pmb{X}\\) is a random vector such that \\(Y=m(\\pmb{X})+e\\), then the following statements are equivalent: 1. \\(m(X)=E[Y|\\pmb{X}]\\), the CEF of \\(Y\\) given \\(\\pmb{X}\\) 2. \\({\\mathbb{E}\\left[ e|\\pmb{X} \\right]}=0\\) "],["best-linear-predictor.html", "1.2 Best Linear Predictor", " 1.2 Best Linear Predictor Let \\(Y\\) be a random variable and \\(\\pmb{X}\\) be a random vector. We denote the best linear predictor of \\(Y\\) given \\(\\pmb{X}\\) by \\(\\mathscr{P}[Y|\\pmb{X}]\\). It’s also called the linear projection of \\(Y\\) on \\(\\pmb{X}\\). Theorem 1.2 (Best Linear Predictor) Under the following assumptions \\(\\mathbb{E}\\left[Y^2\\right]&lt;\\infty\\) \\(\\mathbb{E}||\\pmb{X}||^2&lt;\\infty\\) \\(\\mathbb{Q}_{\\pmb{XX}}:=\\mathbb{E}\\left[\\pmb{X}\\pmb{X}&#39;\\right]\\) is positive-definite the best linear predictor exists uniquely, and has the form \\[ \\mathscr{P}[Y|\\pmb{X}]=\\pmb{X}&#39;\\beta, \\] where \\(\\beta=\\left(\\mathbb{E}[\\pmb{X}\\pmb{X}&#39;]\\right)^{-1}\\mathbb{E}[\\pmb{X}Y]\\). If the BLP exists, the linear projection error \\(e=Y-\\mathscr{P}[Y|\\pmb{X}]\\) follows the following properties: \\(\\mathbb{E}[\\pmb{X}e]=\\pmb{0}\\) \\(\\mathbb{E}[e]=0\\) if \\(\\pmb{X}&#39;=\\begin{bmatrix}1 &amp; X_1 &amp; \\ldots &amp; X_k \\end{bmatrix}\\). "],["ordinary-least-squares.html", "Chapter 2 Ordinary Least Squares", " Chapter 2 Ordinary Least Squares Let \\(Y\\) be our outcome random variable and \\[ \\pmb{X}=\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix}_{(k+1)\\times 1} \\] be our predictor vector containing \\(k\\) predictors and a constant. We denote the joint distribution of \\((Y,\\pmb{X})\\) by \\(F(y,\\pmb{x})\\), i.e., \\[ F(y,\\pmb{x})=\\mathbb{P}(Y\\leq y, \\pmb{X}\\leq\\pmb{x}) =\\mathbb{P}(Y\\leq y,X_1\\leq x_1,\\ldots,X_k\\leq x_k). \\] The dataset or sample is a collection of observations \\(\\{(Y_i,\\pmb{X}_i): i=1,2,\\ldots,n\\}\\). We assume that each observation \\((Y_i,\\pmb{X}_i)\\) is a random vector drawn from the common distribution or population \\(F\\). "],["matrix-formulation.html", "2.1 Matrix Formulation", " 2.1 Matrix Formulation For a given vector of (unknown) coefficients \\(\\pmb{\\beta}=\\begin{bmatrix}\\beta_0 &amp; \\beta_1 &amp; \\ldots &amp; \\beta_k\\end{bmatrix}&#39;\\in\\mathbb{R}^{k+1}\\), we define the following cost function: \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2. \\] The cost function \\(\\widehat{S}({\\pmb{\\beta}})\\) can also be thought of as the average sum of residuals. In fact, \\(\\widehat{S}({\\pmb{\\beta}})\\) is the moment (plug-in) estimator of the mean squared error, \\[ S(\\pmb{\\beta})={\\mathbb{E}\\left[ (Y-\\pmb{X}&#39;\\pmb{\\beta})^2 \\right]}. \\] We now minimize \\(\\widehat{S}({\\pmb{\\beta}})\\) over all possible choices of \\(\\pmb{\\beta}\\in\\mathbb{R}^{k+1}\\). When the minimizer exists and is unique, we call it the least squares estimator, denoted \\(\\widehat{\\pmb{\\beta}}\\). Definition 2.1 ((Ordinary) Least Squares Estimator) The least square estimator is \\[ \\widehat{\\pmb{\\beta}} =\\underset{\\pmb{\\beta}\\in\\mathbb{R}^{k+1}}{\\arg\\min} \\ \\widehat{S}(\\pmb{\\beta}), \\] provided it exists uniquely. "],["solution-of-ols.html", "2.2 Solution of OLS", " 2.2 Solution of OLS We rewrite the cost function as \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}SSE(\\pmb{\\beta}), \\] where \\(SSE(\\pmb{\\beta}):=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2\\). We now express \\(SSE(\\pmb{\\beta})\\) as a quadratic function of \\(\\pmb{\\beta}&#39;\\). \\[ \\begin{align} SSE &amp;=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{X_i}&#39;\\pmb{\\beta}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{\\beta}&#39;\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n \\pmb{\\beta}&#39;(Y_i\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{\\beta}&#39;\\pmb{X_i})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\left(\\sum\\limits_{i=1}^n Y_i^2\\right) - 2\\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + \\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta} \\end{align} \\] Taking partial derivative w.r.t. \\(\\beta_j\\), we get \\[ \\frac{\\partial}{\\partial\\beta_j}SSE(\\pmb{\\beta})=-2\\left[\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right]_j + 2\\left[\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}\\right]_j. \\] Therefore, \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta}) =-2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}. \\] In order to miniminize \\(SSE(\\pmb{\\beta})\\), a necessary condition for \\(\\widehat{\\pmb{\\beta}}\\) is \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta})\\bigg|_{\\pmb{\\beta} =\\widehat{\\pmb{\\beta}}}=\\pmb{0}, \\] i.e., \\[ -2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} =\\pmb{0} \\] So, \\[\\begin{equation} \\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) =\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} \\tag{2.1} \\end{equation}\\] Both the left and right hand side of the above equation are \\(k+1\\) vectors. So, we have a system of \\((k+1)\\) linear equations with \\((k+1)\\) unknowns—the elements of \\(\\pmb{\\beta}\\). Let us define \\[ \\widehat{\\mathbb{Q}}_{\\pmb{XX}} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}\\pmb{X_i}&#39;\\right) \\mbox{ and } \\widehat{\\mathbb{Q}}_{\\pmb{X}Y} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right). \\] Rewriting (2.1), we get \\[ \\begin{equation} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}=\\widehat{\\mathbb{Q}}_{\\pmb{XX}} \\widehat{\\pmb{\\beta}}. \\tag{2.2} \\end{equation} \\] Equation (2.2) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular. In that case, we can solve for \\(\\widehat{\\pmb{\\beta}}\\) to get, \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] To verify that the above choice minimizes \\(SSE(\\pmb{\\beta})\\), one can consider the second-order moment conditions. \\[ \\frac{\\partial^2}{\\partial\\pmb{\\beta}\\partial\\pmb{\\beta}&#39;}SSE(\\pmb{\\beta}) =2\\widehat{\\mathbb{Q}}_{\\pmb{XX}}. \\] If \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, it is also positive-definite. So, we have actually proved the following theorem. Theorem 2.1 If  \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, then the least squares estimator is unique, and is given by \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] "],["least-squares-residuals.html", "2.3 Least Squares Residuals", " 2.3 Least Squares Residuals "],["references.html", "References", " References "],["proofs.html", "A Proofs", " A Proofs "]]
