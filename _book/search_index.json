[["ordinary-least-squares.html", "Chapter 2 Ordinary Least Squares", " Chapter 2 Ordinary Least Squares Let \\(Y\\) be our outcome random variable and \\[ \\pmb{X}=\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix}_{(k+1)\\times 1} \\] be our predictor vector containing \\(k\\) predictors and a constant. We denote the joint distribution of \\((Y,\\pmb{X})\\) by \\(F(y,\\pmb{x})\\), i.e., \\[ F(y,\\pmb{x})=\\mathbb{P}(Y\\leq y, \\pmb{X}\\leq\\pmb{x}) =\\mathbb{P}(Y\\leq y,X_1\\leq x_1,\\ldots,X_k\\leq x_k). \\] The dataset or sample is a collection of observations \\(\\{(Y_i,\\pmb{X}_i): i=1,2,\\ldots,n\\}\\). We assume that each observation \\((Y_i,\\pmb{X}_i)\\) is a random vector drawn from the common distribution or population \\(F\\). "],["matrix-formulation.html", "2.1 Matrix Formulation", " 2.1 Matrix Formulation For a given vector of (unknown) coefficients \\(\\pmb{\\beta}=\\begin{bmatrix}\\beta_0 &amp; \\beta_1 &amp; \\ldots &amp; \\beta_k\\end{bmatrix}&#39;\\in\\mathbb{R}^{k+1}\\), we define the following cost function: \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2. \\] The cost function \\(\\widehat{S}({\\pmb{\\beta}})\\) can also be thought of as the average sum of residuals. In fact, \\(\\widehat{S}({\\pmb{\\beta}})\\) is the moment (plug-in) estimator of the mean squared error, \\[ S(\\pmb{\\beta})={\\mathbb{E}\\left[ (Y-\\pmb{X}&#39;\\pmb{\\beta})^2 \\right]}. \\] We now minimize \\(\\widehat{S}({\\pmb{\\beta}})\\) over all possible choices of \\(\\pmb{\\beta}\\in\\mathbb{R}^{k+1}\\). When the minimizer exists and is unique, we call it the least squares estimator, denoted \\(\\widehat{\\pmb{\\beta}}\\). Definition 2.1 ((Ordinary) Least Squares Estimator) The least square estimator is \\[ \\widehat{\\pmb{\\beta}} =\\underset{\\pmb{\\beta}\\in\\mathbb{R}^{k+1}}{\\arg\\min} \\ \\widehat{S}(\\pmb{\\beta}), \\] provided it exists uniquely. "],["solution-of-ols.html", "2.2 Solution of OLS", " 2.2 Solution of OLS We rewrite the cost function as \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}SSE(\\pmb{\\beta}), \\] where \\(SSE(\\pmb{\\beta}):=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2\\). We now express \\(SSE(\\pmb{\\beta})\\) as a quadratic function of \\(\\pmb{\\beta}&#39;\\). \\[ \\begin{align} SSE &amp;=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{X_i}&#39;\\pmb{\\beta}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{\\beta}&#39;\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n \\pmb{\\beta}&#39;(Y_i\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{\\beta}&#39;\\pmb{X_i})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\left(\\sum\\limits_{i=1}^n Y_i^2\\right) - 2\\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + \\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta} \\end{align} \\] Taking partial derivative w.r.t. \\(\\beta_j\\), we get \\[ \\frac{\\partial}{\\partial\\beta_j}SSE(\\pmb{\\beta})=-2\\left[\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right]_j + 2\\left[\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}\\right]_j. \\] Therefore, \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta}) =-2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}. \\] In order to miniminize \\(SSE(\\pmb{\\beta})\\), a necessary condition for \\(\\widehat{\\pmb{\\beta}}\\) is \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta})\\bigg|_{\\pmb{\\beta} =\\widehat{\\pmb{\\beta}}}=\\pmb{0}, \\] i.e., \\[ -2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} =\\pmb{0} \\] So, \\[\\begin{equation} \\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) =\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} \\tag{2.1} \\end{equation}\\] Both the left and right hand side of the above equation are \\(k+1\\) vectors. So, we have a system of \\((k+1)\\) linear equations with \\((k+1)\\) unknowns—the elements of \\(\\pmb{\\beta}\\). Let us define \\[ \\widehat{\\mathbb{Q}}_{\\pmb{XX}} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}\\pmb{X_i}&#39;\\right) \\mbox{ and } \\widehat{\\mathbb{Q}}_{\\pmb{X}Y} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right). \\] Rewriting (2.1), we get \\[ \\begin{equation} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}=\\widehat{\\mathbb{Q}}_{\\pmb{XX}} \\widehat{\\pmb{\\beta}}. \\tag{2.2} \\end{equation} \\] Equation (2.2) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular. In that case, we can solve for \\(\\widehat{\\pmb{\\beta}}\\) to get, \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] To verify that the above choice minimizes \\(SSE(\\pmb{\\beta})\\), one can consider the second-order moment conditions. \\[ \\frac{\\partial^2}{\\partial\\pmb{\\beta}\\partial\\pmb{\\beta}&#39;}SSE(\\pmb{\\beta}) =2\\widehat{\\mathbb{Q}}_{\\pmb{XX}}. \\] If \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, it is also positive-definite. So, we have actually proved the following theorem. Theorem 2.1 If  \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, then the least squares estimator is unique, and is given by \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] "],["errors-and-residuals.html", "2.3 Errors and Residuals", " 2.3 Errors and Residuals We first define the fitted value as \\[ \\widehat{Y}_i=\\pmb{X}_i&#39;\\widehat{\\pmb{\\beta}}\\mbox{ for } i=1,2,\\ldots,n. \\] For the least squares estimators, we define the errors and residuals in the following way: \\[ e_i=Y_i-\\pmb{X}&#39;\\pmb{\\beta}, \\mbox{ and } \\widehat{e}_i=Y_i-\\widehat{Y}_i. \\] Theorem 2.2 If  \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, then 1. \\(\\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{e}_i=\\pmb{0}\\) 2. \\(\\sum\\limits_{i=1}^n\\widehat{e}_i=0\\) Proof. \\[\\begin{align} \\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{e}_i &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_i(Y_i-\\widehat{Y}_i) \\\\ &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i-\\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{Y}_i \\\\ &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i-\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\pmb{\\widehat{\\beta}} \\\\ &amp;=\\widehat{Q}_{\\pmb{X}Y}-\\widehat{Q}_{\\pmb{XX}}\\pmb{\\widehat{\\beta}} \\\\ &amp;=\\widehat{Q}_{\\pmb{X}Y}-\\widehat{Q}_{\\pmb{XX}} \\left( \\widehat{Q}_{\\pmb{XX}}^{-1} \\widehat{Q}_{\\pmb{X}Y} \\right) \\\\ &amp;=\\pmb{0} \\end{align}\\] From the first row of (1) we get \\[ \\sum\\limits_{i=1}^n X_{i1}\\widehat{e}_i=0. \\] Since \\(X_{i1}=1\\) for all \\(i\\), we have that \\[ \\sum\\limits_{i=1}^n\\widehat{e}_i=0. \\] Hence the result. "],["model-in-matrix-notation.html", "2.4 Model in Matrix Notation", " 2.4 Model in Matrix Notation Taking the definition of errors from the last section, we can write down a system of \\(n\\) linear equations: \\[\\begin{align} Y_1 &amp;= \\pmb{X_1}&#39;\\pmb{\\beta} + e_1 \\\\ Y_2 &amp;= \\pmb{X_2}&#39;\\pmb{\\beta} + e_2 \\\\ &amp; \\vdots \\\\ Y_n &amp;= \\pmb{X_1}&#39;\\pmb{\\beta} + e_n \\end{align}\\] Define \\[ \\pmb{Y}=\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}_{n\\times1},\\ \\mathbb{X}=\\begin{bmatrix} \\pmb{X}_1 \\\\ \\pmb{X}_2 \\\\ \\vdots \\\\ \\pmb{X}_n \\end{bmatrix}_{n\\times(k+1)}, \\mbox{ and } \\pmb{e}=\\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}_{n\\times1}. \\] We can now rewrite the system as the following: \\[ \\pmb{Y}=\\mathbb{X}\\pmb{\\beta}+\\pmb{e}. \\] Note that \\[ \\mathbb{X}=\\begin{bmatrix} 1 &amp; X_{11} &amp; X_{12} &amp; \\ldots &amp; X_{1k} \\\\ 1 &amp; X_{21} &amp; X_{22} &amp; \\ldots &amp; X_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{n1} &amp; X_{n2} &amp; \\ldots &amp; X_{nk} \\end{bmatrix} \\] We also note that \\[ \\widehat{Q}_{\\pmb{XX}}=\\sum\\limits_{i=1}^n\\pmb{X}_i&#39;\\pmb{X}_i= \\mathbb{X}&#39;\\mathbb{X}, \\] and \\[ \\widehat{Q}_{\\pmb{X}Y}=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i= \\mathbb{X}&#39;\\pmb{Y}. \\] So, we have write the least squares estimator as \\[ \\widehat{\\pmb{\\beta}}=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}\\pmb{Y}. \\] Similarly, the residual vector is \\[ \\widehat{\\pmb{e}}=\\pmb{Y}-\\mathbb{X}\\widehat{\\pmb{\\beta}}. \\] As a consequence, we can write \\[ \\mathbb{X}&#39;\\widehat{\\pmb{e}}=\\pmb{0}. \\] "]]
