% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{charter}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={w203: Statistics for Data Science},
  pdfauthor={w203 Instructors},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{w203: Statistics for Data Science}
\author{w203 Instructors}
\date{2022-06-18}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\E}[1]{{\mathbb{E}\left[ #1 \right]}}
\newcommand{\V}[1]{{\mathbb{V}\left[ #1 \right]}}
\renewcommand{\C}[1]{{\text{Cov}\left[ #1 \right]}}
\renewcommand{\v}[1]{{\boldsymbol{#1}}}
\newcommand{\m}[1]{{\mathbb{#1}}}
\newcommand{\p}[1]{{\mathbb{P}\left(#1\right)}}
\newcommand{\eps}{\varepsilon}

\hypertarget{cover}{%
\chapter*{Cover}\label{cover}}
\addcontentsline{toc}{chapter}{Cover}

\includegraphics[width=0.85\linewidth]{./images/cover}

\hypertarget{part-probability-theory}{%
\part{Probability Theory}\label{part-probability-theory}}

\hypertarget{probability-spaces}{%
\chapter{Probability Spaces}\label{probability-spaces}}

\hypertarget{kolmogorovs-axioms}{%
\section{Kolmogorov's Axioms}\label{kolmogorovs-axioms}}

\hypertarget{conditional-probability}{%
\section{Conditional Probability}\label{conditional-probability}}

\hypertarget{random-variables}{%
\chapter{Random Variables}\label{random-variables}}

\hypertarget{part-learning-from-data}{%
\part{Learning from Data}\label{part-learning-from-data}}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

\hypertarget{regression}{%
\chapter{Regression}\label{regression}}

We write a \(k\)-vector (of scalars) as a row
\[
{\boldsymbol{x}}=
\begin{bmatrix}
x_1 &
x_2 &
\ldots &
x_k
\end{bmatrix}.
\]
The transpose of \({\boldsymbol{x}}\) as
\[
{\boldsymbol{x}}^T=
\begin{bmatrix}
x_1 \\ x_2\\ \vdots \\ x_k
\end{bmatrix}.
\]
We use uppercase letters \(X,Y,Z,\ldots\) to denote random variables. Random vectors are denoted by \textbf{bold} uppercase letters \({\boldsymbol{X}},{\boldsymbol{Y}},{\boldsymbol{Z}},\ldots\), and written as a row vector. For example, \[
{\boldsymbol{X}}=
\begin{bmatrix}
X_{[1]} &
X_{[2]} &
\ldots &
X_{[k]}
\end{bmatrix}.
\]
In order to distinguish random matrices from vectors, a random matrix is denoted by \({\mathbb{X}}\).

The expectation of \({\boldsymbol{X}}\) is defined as
\[
{\mathbb{E}\left[ {\boldsymbol{X}} \right]}=
\begin{bmatrix}
{\mathbb{E}\left[ X_{[1]} \right]} &
{\mathbb{E}\left[ X_{[2]} \right]} &
\ldots &
{\mathbb{E}\left[ X_{[k]} \right]}
\end{bmatrix}.
\]
The \(k\times k\) covariance matrix of \({\boldsymbol{X}}\) is defined as
\[
\begin{aligned}
{\mathbb{V}\left[ {\boldsymbol{X}} \right]} &={\mathbb{E}\left[ ({\boldsymbol{X}}-{\mathbb{E}\left[ {\boldsymbol{X}} \right]})^T({\boldsymbol{X}}-{\mathbb{E}\left[ {\boldsymbol{X}} \right]}) \right]} \\
&=\begin{bmatrix}
\sigma_1^2 & \sigma_{12} & \ldots & \sigma_{1k} \\
\sigma_{21} & \sigma_{2}^2 & \ldots & \sigma_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{k1} & \sigma_{k2}^2 & \ldots & \sigma_{k}^2 \\
\end{bmatrix}_{k\times k}
\end{aligned}
\]

where \(\sigma_j={\mathbb{V}\left[ X_{[j]} \right]}\) and \(\sigma_{ij}={\text{Cov}\left[ X_{[i]},X_{[j]} \right]}\) for \(i,j=1,2,\ldots,k\) and \(i\neq j\).

\begin{theorem}[Linearity of Exectation]
\protect\hypertarget{thm:explin}{}\label{thm:explin}Let \({\mathbb{A}}_{l\times k},{\mathbb{B}}_{m\times l}\) be fixed matrices and \({\boldsymbol{c}}\) a fixed vector of size \(l\). If \({\boldsymbol{X}}\) and \({\boldsymbol{Y}}\) are random vectors of size \(k\) and \(m\), respectively, such that \({\mathbb{E}\left[ X \right]}<\infty,{\mathbb{E}\left[ Y \right]}<\infty\), then
\[
{\mathbb{E}\left[ {\mathbb{A}}{\boldsymbol{X}}+{\boldsymbol{Y}}{\mathbb{B}}+{\boldsymbol{c}} \right]}={\mathbb{A}}{\mathbb{E}\left[ {\boldsymbol{X}} \right]}+{\mathbb{E}\left[ {\boldsymbol{Y}} \right]}{\mathbb{B}}+{\boldsymbol{c}}.
\]
\end{theorem}

\hypertarget{conditional-expectation-function}{%
\section{Conditional Expectation Function}\label{conditional-expectation-function}}

\begin{theorem}[Characterization of CEF]
\protect\hypertarget{thm:cef}{}\label{thm:cef}If ~\({\mathbb{E}\left[ Y^2 \right]}<\infty\) and \({\boldsymbol{X}}\) is a random vector such that \(Y=m({\boldsymbol{X}})+e\), then the following statements are equivalent:\\
1. \(m({\boldsymbol{X}})={\mathbb{E}\left[ Y|{\boldsymbol{X}} \right]}\), the CEF of \(Y\) given \({\boldsymbol{X}}\)\\
2. \({\mathbb{E}\left[ e|{\boldsymbol{X}} \right]}=0\)
\end{theorem}

\hypertarget{best-linear-predictor}{%
\section{Best Linear Predictor}\label{best-linear-predictor}}

Let \(Y\) be a random variable and \({\boldsymbol{X}}\) be a random vector of \(k\) variables. We denote the \textbf{best linear predictor} of \(Y\) given \({\boldsymbol{X}}\) by \(\mathscr{P}[Y|{\boldsymbol{X}}]\). It's also called the \textbf{linear projection} of \(Y\) on \({\boldsymbol{X}}\).

\begin{theorem}[Best Linear Predictor]
\protect\hypertarget{thm:blp}{}\label{thm:blp}Under the following assumptions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \({\mathbb{E}\left[ Y^2 \right]}<\infty\)
\item
  \({\mathbb{E}\left[ ||\bf{X}||^2 \right]}<\infty\)
\item
  \({\mathbb{Q}}_{\bf{XX}}\stackrel{\text{def}}{=}{\mathbb{E}\left[ {\boldsymbol{X}}^T{\boldsymbol{X}} \right]}\) is positive-definite
\end{enumerate}

the best linear predictor exists uniquely, and has the form
\[
\mathscr{P}[Y|{\boldsymbol{X}}]={\boldsymbol{X}}{\boldsymbol{\beta}},
\]
where \({\boldsymbol{\beta}}=\left({\mathbb{E}\left[ {\boldsymbol{X}}^T{\boldsymbol{X}} \right]}\right)^{-1}{\mathbb{E}}[{\boldsymbol{X}}^TY]\) is a column vector.
\end{theorem}

In the following theorem, we show that the BLP error is \emph{uncorrelated} to the explanatory variables.

\begin{theorem}[Best Linear Predictor Error]
\protect\hypertarget{thm:blperror}{}\label{thm:blperror}

If the BLP exists, the linear projection error \(\varepsilon=Y-\mathscr{P}[Y|{\boldsymbol{X}}]\) follows the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \({\mathbb{E}}[{\boldsymbol{X}}^T\varepsilon]={\boldsymbol{0}}\)
\item
  moreover, \({\mathbb{E}}[\varepsilon]=0\) if
  \({\boldsymbol{X}}=\begin{bmatrix}1 & X_{[1]} & \ldots & X_{[k]} \end{bmatrix}\) contains a constant.
\end{enumerate}

\end{theorem}

\hypertarget{ordinary-least-squares}{%
\chapter{Ordinary Least Squares}\label{ordinary-least-squares}}

Let \(Y\) be our outcome random variable and
\[
{\boldsymbol{X}}=\begin{bmatrix}
1 & X_{[1]} & X_{[2]} & \ldots & X_{[k]}
\end{bmatrix}
\]
be our predictor (or explanatory) vector containing \(k\) predictors and a constant. We denote the joint distribution of \((Y,{\boldsymbol{X}})\) by \(F(y,{\boldsymbol{x}})\), i.e.,
\[
F(y,{\boldsymbol{x}})={\mathbb{P}\left(Y\leq y, {\boldsymbol{X}}\leq{\boldsymbol{x}}\right)}
={\mathbb{P}\left(Y\leq y,X_1\leq x_1,\ldots,X_k\leq x_k\right)}.
\]

The \textbf{dataset} or \textbf{sample} is a collection of observations \(\{(Y_i,{\boldsymbol{X}}_i): i=1,2,\ldots,n\}\). We assume that each observation
\((Y_i,{\boldsymbol{X}}_i)\) is a random (row) vector drawn from the common distribution, sometimes referred to as the \textbf{population}, \(F\).

For a given vector of (unknown) coefficients
\({\boldsymbol{\beta}}=\begin{bmatrix}\beta_0 & \beta_1 & \ldots & \beta_k\end{bmatrix}^T\in\mathbb{R}^{k+1}\), we define the following \textbf{cost function}:
\[
\widehat{S}({\boldsymbol{\beta}})=\frac{1}{n}\sum\limits_{i=1}^n(Y_i-{\boldsymbol{X_i}}{\boldsymbol{\beta}})^2.
\]
The cost function \(\widehat{S}({{\boldsymbol{\beta}}})\) can also be thought of as the average sum of residuals. In fact, \(\widehat{S}({{\boldsymbol{\beta}}})\) is the moment (plug-in) estimator of the mean squared error,
\[
S({\boldsymbol{\beta}})={\mathbb{E}\left[ (Y-{\boldsymbol{X}}{\boldsymbol{\beta}})^2 \right]}.
\]

We now minimize \(\widehat{S}({{\boldsymbol{\beta}}})\) over all possible choices of
\({\boldsymbol{\beta}}\in\mathbb{R}^{k+1}\). When the minimizer exists and is unique, we call it the \textbf{least squares estimator}, denoted \(\widehat{{\boldsymbol{\beta}}}\).

\begin{definition}[(Ordinary) Least Squares Estimator]
The least square estimator is \[
\widehat{{\boldsymbol{\beta}}}
=\underset{{\boldsymbol{\beta}}\in\mathbb{R}^{k+1}}{\arg\min} \ \widehat{S}({\boldsymbol{\beta}}),
\]
provided it exists uniquely.
\end{definition}

\hypertarget{solution-of-ols}{%
\section{Solution of OLS}\label{solution-of-ols}}

We rewrite the cost function as
\[
\widehat{S}({\boldsymbol{\beta}})=\frac{1}{n}SSE({\boldsymbol{\beta}}),
\]
where
\(SSE({\boldsymbol{\beta}})\stackrel{\text{def}}{=}\sum\limits_{i=1}^n(Y_i-{\boldsymbol{X_i}}{\boldsymbol{\beta}})^2\).

We now express \(SSE({\boldsymbol{\beta}})\) as a quadratic function of \({\boldsymbol{\beta}}\).
\[
\begin{aligned}
SSE &=\sum\limits_{i=1}^n(Y_i-{\boldsymbol{X_i}}{\boldsymbol{\beta}})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i({\boldsymbol{X_i}}{\boldsymbol{\beta}})
  + \sum\limits_{i=1}^n ({\boldsymbol{X_i}}{\boldsymbol{\beta}})^2 \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n Y_i({\boldsymbol{\beta}}^T{\boldsymbol{X_i}}^T)
  + \sum\limits_{i=1}^n ({\boldsymbol{X_i}}{\boldsymbol{\beta}})({\boldsymbol{X_i}}{\boldsymbol{\beta}}) \\
&=\sum\limits_{i=1}^n Y_i^2 
  - 2\sum\limits_{i=1}^n {\boldsymbol{\beta}}^T(Y_i{\boldsymbol{X_i}}^T)
  + \sum\limits_{i=1}^n ({\boldsymbol{\beta}}^T{\boldsymbol{X_i}}^T)({\boldsymbol{X_i}}{\boldsymbol{\beta}}) \\
&=\left(\sum\limits_{i=1}^n Y_i^2\right) 
  - 2{\boldsymbol{\beta}}^T\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right)
  + {\boldsymbol{\beta}}^T\left(\sum\limits_{i=1}^n {\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right){\boldsymbol{\beta}}
\end{aligned}
\]
Taking partial derivative w.r.t. \(\beta_j\), we get
\[
\frac{\partial}{\partial\beta_j}SSE({\boldsymbol{\beta}})=-2\left[\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right]_j 
+ 2\left[\left(\sum\limits_{i=1}^n  {\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right){\boldsymbol{\beta}}\right]_j.
\]

Therefore,
\[
\frac{\partial}{\partial{\boldsymbol{\beta}}}SSE({\boldsymbol{\beta}})
=-2\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right) 
+ 2\left(\sum\limits_{i=1}^n  {\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right){\boldsymbol{\beta}}.
\]

In order to miniminize \(SSE({\boldsymbol{\beta}})\), a necessary condition for \(\widehat{{\boldsymbol{\beta}}}\) is
\[
\frac{\partial}{\partial{\boldsymbol{\beta}}}SSE({\boldsymbol{\beta}})\bigg|_{{\boldsymbol{\beta}}
=\widehat{{\boldsymbol{\beta}}}}={\boldsymbol{0}},
\]
i.e.,
\[
-2\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right) 
+ 2\left(\sum\limits_{i=1}^n  {\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right)\widehat{{\boldsymbol{\beta}}}
={\boldsymbol{0}}
\]
So,
\begin{equation}
\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right)
=\left(\sum\limits_{i=1}^n  {\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right)\widehat{{\boldsymbol{\beta}}}
\label{eq:moment-0}
\end{equation}

Both the left and right hand side of the above equation are \(k+1\) vectors. So, we have a system of \((k+1)\) linear equations with \((k+1)\) unknowns---the elements of \({\boldsymbol{\beta}}\).

Let us define

\[
\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}
=\frac{1}{n}\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^T{\boldsymbol{X_i}}\right)
\mbox{ and }
\widehat{\mathbb{Q}}_{{\boldsymbol{X}}Y}
=\frac{1}{n}\left(\sum\limits_{i=1}^n{\boldsymbol{X_i}}^TY_i\right).
\]

Rewriting \eqref{eq:moment-0}, we get
\begin{equation}
\widehat{\mathbb{Q}}_{{\boldsymbol{X}}Y}=\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}
\widehat{{\boldsymbol{\beta}}}.
\label{eq:moment}
\end{equation}

Equation \eqref{eq:moment} is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that
\(\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\) is non-singular. In that case, we can solve for \(\widehat{{\boldsymbol{\beta}}}\) to get,
\[
\widehat{{\boldsymbol{\beta}}}=\left[\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\right]^{-1}
\widehat{\mathbb{Q}}_{{\boldsymbol{X}}Y}.
\]
To verify that the above choice minimizes \(SSE({\boldsymbol{\beta}})\),
one can consider the second-order moment conditions.
\[
\frac{\partial^2}{\partial{\boldsymbol{\beta}}\partial{\boldsymbol{\beta}}^T}SSE({\boldsymbol{\beta}})
=2\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}.
\]
If \(\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\) is non-singular, it is also positive-definite. So, we have actually proved the following theorem.

\begin{theorem}
If ~\(\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\) is non-singular, then the
least squares estimator is unique, and is given by
\[
\widehat{{\boldsymbol{\beta}}}=\left[\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\right]^{-1}
\widehat{\mathbb{Q}}_{{\boldsymbol{X}}Y}.
\]
\end{theorem}

\hypertarget{errors-and-residuals}{%
\section{Errors and Residuals}\label{errors-and-residuals}}

Recall that \({\boldsymbol{\beta}}\) denotes the coefficients of the best linear predictor \ref{thm:blp}. We first define the \textbf{fitted} value as
\[
\widehat{Y}_i={\boldsymbol{X}}_i\widehat{{\boldsymbol{\beta}}}\mbox{ for }
i=1,2,\ldots,n.
\]
For the least squares estimators, we define the \textbf{errors} and \textbf{residuals} in the following way:
\[
\varepsilon_i=Y_i-{\boldsymbol{X}}_i{\boldsymbol{\beta}}, \mbox{ and } 
e_i=Y_i-\widehat{Y}_i.
\]

\begin{theorem}[Least Squares Error]
\protect\hypertarget{thm:olserror}{}\label{thm:olserror}If ~\(\widehat{\mathbb{Q}}_{{\boldsymbol{XX}}}\) is non-singular, then\\
1. \(\sum\limits_{i=1}^n{\boldsymbol{X}}_i^Te_i={\boldsymbol{0}}\)\\
2. \(\sum\limits_{i=1}^ne_i=0\)
\end{theorem}

\begin{proof}
\[
\begin{aligned}
\sum\limits_{i=1}^n{\boldsymbol{X}}_i^Te_i 
&=\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T(Y_i-\widehat{Y}_i) \\
&=\sum\limits_{i=1}^n{\boldsymbol{X}}_i^TY_i-\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T\widehat{Y}_i \\
&=\sum\limits_{i=1}^n{\boldsymbol{X}}_i^TY_i-\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i{\boldsymbol{\widehat{\beta}}} \\
&=\widehat{{\mathbb{Q}}}_{{\boldsymbol{X}}Y}-\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}{\boldsymbol{\widehat{\beta}}} \\
&=\widehat{{\mathbb{Q}}}_{{\boldsymbol{X}}Y}-\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}
\left( \widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1} \widehat{{\mathbb{Q}}}_{{\boldsymbol{X}}Y} \right) \\
&={\boldsymbol{0}}
\end{aligned}
\]
From the first row of (1) we get

\[
\sum\limits_{i=1}^n e_i=0.
\]
Hence the result.
\end{proof}

\hypertarget{matrix-notation}{%
\section{Matrix Notation}\label{matrix-notation}}

Taking the definition of errors from the last section, we can write down a system of \(n\) linear equations:
\[
\begin{aligned}
Y_1 &= {\boldsymbol{X_1}}{\boldsymbol{\beta}} + \varepsilon_1 \\
Y_2 &= {\boldsymbol{X_2}}{\boldsymbol{\beta}} + \varepsilon_2 \\
& \vdots \\
Y_n &= {\boldsymbol{X_n}}{\boldsymbol{\beta}} + \varepsilon_n
\end{aligned}
\]

Define
\[
{\boldsymbol{Y}}=\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}_{n\times1},\ 
\mathbb{X}=\begin{bmatrix}
1 & X_{[1]1} & X_{[2]1} & \ldots & X_{[k]1} \\
1 & X_{[1]2} & X_{[2]2} & \ldots & X_{[k]2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & X_{[1]n} & X_{[2]n} & \ldots & X_{[k]n}
\end{bmatrix},\mbox{ and }
{\boldsymbol{\varepsilon}}=\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}_{n\times1}.
\]
We can now rewrite the system as the following:
\[
{\boldsymbol{Y}}={\mathbb{X}}{\boldsymbol{\beta}}+{\boldsymbol{\varepsilon}}.
\]
We also note that
\[
\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}=\sum\limits_{i=1}^n{\boldsymbol{X}}^T_i{\boldsymbol{X}}_i=
{\mathbb{X}}^T{\mathbb{X}},
\]
and
\[
\widehat{{\mathbb{Q}}}_{{\boldsymbol{X}}Y}=\sum\limits_{i=1}^n{\boldsymbol{X}}_i^TY_i=
{\mathbb{X}}^T{\boldsymbol{Y}}.
\]
So, we have write the least squares estimator as
\[
\widehat{{\boldsymbol{\beta}}}=\left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T{\boldsymbol{Y}}.
\]
Similarly, the residual vector is
\[
{\boldsymbol{e}}={\boldsymbol{Y}}-{\mathbb{X}}\widehat{{\boldsymbol{\beta}}}.
\]
As a consequence of \ref{thm:olserror}, we can write
\[
{\mathbb{X}}^T{\boldsymbol{e}}={\boldsymbol{0}}.
\]

\hypertarget{linear-conditional-expectation-function}{%
\chapter{Linear Conditional Expectation Function}\label{linear-conditional-expectation-function}}

\hypertarget{variance-of-error}{%
\section{Variance of Error}\label{variance-of-error}}

We first compute the (unconditional) variance of the error vector \(\pmb{e}\). The covariance matrix
\[
\mathbb{V}[\pmb{e}]={\mathbb{E}\left[ \pmb{e}\pmb{e}' \right]}-{\mathbb{E}\left[ \pmb{e} \right]}{\mathbb{E}\left[ \pmb{e}' \right]}={\mathbb{E}\left[ \pmb{e}\pmb{e}' \right]}\stackrel{\text{def}}{=}\mathbb{D}.
\]
For \(i\neq j\), the errors \(e_i\),\(e_j\) are independent. As a result, \({\mathbb{E}\left[ e_ie_j \right]}={\mathbb{E}\left[ e_i \right]}{\mathbb{E}\left[ e_j \right]}=0\). So, \(\mathbb{D}\) is a diagonal matrix with the \(i\)-th diagonal element \(\sigma_i^2\):
\[
\mathbb{D}=\begin{bmatrix}
\sigma_1^2 & 0 & \ldots & 0 \\
0 & \sigma_2^2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \sigma_n^2
\end{bmatrix}.
\]

\hypertarget{variance-of-ols-estimators}{%
\section{Variance of OLS Estimators}\label{variance-of-ols-estimators}}

\hypertarget{large-sample-regression}{%
\chapter{Large-Sample Regression}\label{large-sample-regression}}

We assume that the best linear predictor, \(\mathscr{P}[Y|{\boldsymbol{X}}]\), of \(Y\) given \({\boldsymbol{X}}\) is \({\boldsymbol{X}}{\boldsymbol{\beta}}\).
\[
Y={\boldsymbol{X}}{\boldsymbol{\beta}}+\varepsilon.
\]
We have from Theorem \ref{thm:blperror}
\[{\mathbb{E}\left[ \varepsilon \right]}=0,\mbox{ and }{\mathbb{E}\left[ {\boldsymbol{X}}^T\varepsilon \right]}={\boldsymbol{0}}.\]

We also assume that the dataset \(\{(Y_i,{\boldsymbol{X}}_i)\}\) is taken \textbf{i.i.d.} from the joint distribution of \((Y,{\boldsymbol{X}})\). For each \(i\), we can write
\[
Y_i={\boldsymbol{X_i}}{\boldsymbol{\beta}}+\varepsilon_i.
\]
In matrix notation, we can write
\[
{\boldsymbol{Y}}={\mathbb{X}}{\boldsymbol{\beta}}+{\boldsymbol{\varepsilon}}.
\]
Then
\[{\mathbb{E}\left[ {\boldsymbol{\varepsilon}} \right]}={\boldsymbol{0}},\text{ and } {\mathbb{E}\left[ {\boldsymbol{\varepsilon}} \right]}={\boldsymbol{0}}\]

\hypertarget{consistency-of-ols-estimators}{%
\section{Consistency of OLS Estimators}\label{consistency-of-ols-estimators}}

\hypertarget{asymptotic-normality}{%
\section{Asymptotic Normality}\label{asymptotic-normality}}

We start by revealing an alternative expression for the OLS estimators \(\widehat{{\boldsymbol{\beta}}}\) using matrix notation.

\[
\begin{aligned}
\widehat{{\boldsymbol{\beta}}}
&=\left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T{\boldsymbol{Y}} \\
&=\left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T({\mathbb{X}}{\boldsymbol{\beta}}+{\boldsymbol{\varepsilon}}) \\
&=\left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}({\mathbb{X}}^T{\mathbb{X}}){\boldsymbol{\beta}}+
\left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T{\boldsymbol{\varepsilon}} \\
&={\boldsymbol{\beta}} + \left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T{\boldsymbol{\varepsilon}}
\end{aligned}
\]

So,
\begin{equation}
\widehat{{\boldsymbol{\beta}}}-{\boldsymbol{\beta}} = \left[{\mathbb{X}}^T{\mathbb{X}}\right]^{-1}{\mathbb{X}}^T{\boldsymbol{\varepsilon}}
\label{eq:beta}
\end{equation}

We can then multiply by \(\sqrt{n}\) both sides of Equation \eqref{eq:beta} to get
\[
\begin{aligned}
\sqrt{n}\left(\widehat{{\boldsymbol{\beta}}}-{\boldsymbol{\beta}}\right)
&=\left( \frac{1}{n}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i \right)^{-1}
\left( \frac{1}{\sqrt{n}}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T\varepsilon_i \right) \\
&=\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1}
\left( \frac{1}{\sqrt{n}}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T\varepsilon_i \right)
\end{aligned}
\]
From the consistency of OLS estimators, we already have
\[ \widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}\xrightarrow[p]{\quad\quad}{\mathbb{Q}}_{{\boldsymbol{XX}}}\]
Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression.

We first note (from i.i.d. and Theorem \ref{thm:blperror}) that
\[
{\mathbb{E}\left[ {\boldsymbol{X}}_i^T\varepsilon_i \right]}={\mathbb{E}\left[ {\boldsymbol{X}}^T\varepsilon \right]}={\boldsymbol{0}}.
\]
Let us compute the covariance matrix of \({\boldsymbol{X}}_i\varepsilon_i\). Since the expectation vector is zero, we have
\[
{\mathbb{V}}[{\boldsymbol{X}}_i^T\varepsilon_i]={\mathbb{E}\left[ {\boldsymbol{X}}_i^T\varepsilon_i\left({\boldsymbol{X}}_i^T\varepsilon_i\right)^T \right]}={\mathbb{E}\left[ {\boldsymbol{X}}^T{\boldsymbol{X}}\varepsilon^2 \right]}\stackrel{\text{def}}{=}{\mathbb{A}}.
\]
As any function of \(\{(Y_i,{\boldsymbol{X}}_i)\}\)'s are independent, \(\{{\boldsymbol{X}}_i\varepsilon_i\}\)'s are independent. By the (multivariate) \textbf{Central Limit Theorem}, as \(n\to\infty\)
\[
\frac{1}{\sqrt{n}}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T\varepsilon_i
\xrightarrow[d]{\quad\quad}\mathcal{N}({\boldsymbol{0}},{\mathbb{A}}).
\]
There is a small technicality here, we must have \({\mathbb{A}}<\infty\). This can be imposed by a stronger regularity condition on the moments, e.g.,
\({\mathbb{E}\left[ Y^4 \right]},{\mathbb{E}\left[ ||{\boldsymbol{X}}||^4 \right]}<\infty\).
Putting everything together, we conclude
\[
\sqrt{n}(\widehat{{\boldsymbol{\beta}}}-{\boldsymbol{\beta}})\xrightarrow[d]{\quad\quad}
{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}\mathcal{N}({\boldsymbol{0}},{\mathbb{A}})
=\mathcal{N}\left({\boldsymbol{0}},\left[{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}\right]^T{\mathbb{A}}{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}\right)
=\mathcal{N}\left({\boldsymbol{0}},{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}{\mathbb{A}}{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}\right)
\]

\begin{theorem}[Asymptotic Distribution of OLS Estimators]
\protect\hypertarget{thm:asympvar}{}\label{thm:asympvar}We assume the following:\\
1. The observations \(\{(Y_i,{\boldsymbol{X}}_i)\}_{i=1}^n\) are i.i.d from the joint
distribution of \((Y,{\boldsymbol{X}})\)\\
2. \({\mathbb{E}\left[ Y^4 \right]}<\infty\)\\
3. \({\mathbb{E}\left[ ||{\boldsymbol{X}}||^4 \right]}<\infty\)\\
4. \({\mathbb{Q}}_{{\boldsymbol{XX}}}={\mathbb{E}\left[ {\boldsymbol{X}}{\boldsymbol{X}}' \right]}\) is positive-definite.
Under these assumptions, as \(n\to\infty\)
\[
\sqrt{n}(\widehat{{\boldsymbol{\beta}}}-{\boldsymbol{\beta}})\xrightarrow[d]{\quad\quad}
\mathcal{N}\left({\boldsymbol{0}},{\mathbb{V}}_{{\boldsymbol{\beta}}}\right),
\]
where
\[{\mathbb{V}}_{{\boldsymbol{\beta}}}\stackrel{\text{def}}{=}{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}{\mathbb{A}}{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}\]
and \({\mathbb{Q}}_{{\boldsymbol{XX}}}={\mathbb{E}\left[ {\boldsymbol{X}}^T{\boldsymbol{X}} \right]}\), \({\mathbb{A}}={\mathbb{E}\left[ {\boldsymbol{X}}^T{\boldsymbol{X}}\varepsilon^2 \right]}\).
\end{theorem}

The covariance matrix \({\mathbb{V}}_{{\boldsymbol{\beta}}}\) is called the \textbf{asymptotic variance matrix} of \(\widehat{{\boldsymbol{\beta}}}\). The matrix is sometimes referred to as the \textbf{sandwich} form.

\hypertarget{covariance-matrix-estimation}{%
\section{Covariance Matrix Estimation}\label{covariance-matrix-estimation}}

We now turn our attention to the estimation of the sandwich matrix using a finite sample.

\hypertarget{heteroskedastic-variance}{%
\subsection{Heteroskedastic Variance}\label{heteroskedastic-variance}}

Theorem \ref{thm:asympvar} presented the asymptotic covariance matrix of
\(\sqrt{n}(\widehat{{\boldsymbol{\beta}}}-{\boldsymbol{\beta}})\) is
\[{\mathbb{V}}_{{\boldsymbol{\beta}}}
={\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}{\mathbb{A}}{\mathbb{Q}}_{{\boldsymbol{XX}}}^{-1}.\]
Without imposing any homoskedasticity condition, we estimate \({\mathbb{V}}_{{\boldsymbol{\beta}}}\) using a plug-in estimator.

We have already seen that \(\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}=\frac{1}{n}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i\) is a natural estimator for \({\mathbb{Q}}_{{\boldsymbol{XX}}}\). For \({\mathbb{A}}\), we use the moment estimator
\[
\widehat{{\mathbb{A}}}=\frac{1}{n}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_ie_i^2,
\]
where \(e_i=(Y_i-{\boldsymbol{X}}_i\widehat{{\boldsymbol{\beta}}})\) is the \(i\)-th residual. As it turns out, \(\widehat{{\mathbb{A}}}\) is a consistent estimator
for \({\mathbb{A}}\).

As a result, we get the following plug-in estimator for \({\mathbb{V}}_{{\boldsymbol{\beta}}}\):
\[
\widehat{{\mathbb{V}}}_{{\boldsymbol{\beta}}}=
\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1}\widehat{{\mathbb{A}}}\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1}
\]
The estimator is also consistent. For a proof, see Hensen 2013.

As a consequence, we can get the following estimator for the variance, \({\mathbb{V}}_{\widehat{{\boldsymbol{\beta}}}}\), of \(\widehat{{\boldsymbol{\beta}}}\) in the heteroskedastic case.
\[
\begin{aligned}
\widehat{{\mathbb{V}}}\left[\widehat{{\boldsymbol{\beta}}}\right]
&=\frac{1}{n}\widehat{{\mathbb{V}}}_{{\boldsymbol{\beta}}}^{\text{HC0}} \\
&=\frac{1}{n}\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1}\widehat{{\mathbb{A}}}\widehat{{\mathbb{Q}}}_{{\boldsymbol{XX}}}^{-1} \\
&=\frac{1}{n}\left(\frac{1}{n}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i\right)^{-1}
\left(\frac{1}{n}\sum\limits_{i=1}^ne_i^2{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i\right)
\left(\frac{1}{n}\sum\limits_{i=1}^n{\boldsymbol{X}}_i^T{\boldsymbol{X}}_i\right)^{-1} \\
&=\left({\mathbb{X}}^T{\mathbb{X}}\right)^{-1}
{\mathbb{X}}^T{\mathbb{D}}{\mathbb{X}}
\left({\mathbb{X}}^T{\mathbb{X}}\right)^{-1}
\end{aligned}
\]
where \({\mathbb{D}}\) is an \(n\times n\) diagonal matrix with diagonal entries \(e_1^2,e_2^2,\ldots,e_n^2\).
The estimator, \(\widehat{{\mathbb{V}}}\left[\widehat{{\boldsymbol{\beta}}}\right]\), is referred to as the \textbf{robust error variance estimator} for the OLS coefficients \(\widehat{{\boldsymbol{\beta}}}\).

\hypertarget{homeskedastic-variance}{%
\subsection{Homeskedastic Variance}\label{homeskedastic-variance}}

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{matrix-algebra}{%
\chapter{Matrix Algebra}\label{matrix-algebra}}

In this book, we reserve \textbf{boldface} letter to denote vectors (of scalars and random variables), and ``blackboard bold'' typeface to denote matrices.

We always write a vector as a column
\[
\pmb{v}=\begin{bmatrix}
v_1 \\
v_2 \\
\vdots \\
v_k
\end{bmatrix}_{k\times1}
\]

\begin{definition}[Transpose of a Matrix]
\protect\hypertarget{def:mattrans}{}\label{def:mattrans}Let \(\mathbb{A}_{k\times l}\) be a matrix, it's transpose, denoted \(\mathbb{A}^T\), is an \(l\times k\) matrix such that
the \((i,j)\)-th entry of \(\mathbb{A}\) becomes the \((j,i)\)-th entry of \(\mathbb{A}^T\).
\end{definition}

\begin{definition}[Sum of Matrices]
\protect\hypertarget{def:matsum}{}\label{def:matsum}Let \(\mathbb{A},\mathbb{B}\) are matrices both of size \(k\times l\), then the sum \(\mathbb{A}+\mathbb{B}\) is defined as the another matrix \(\mathbb{C}\) size \(k\times l\) such that the \((i,j)\)-th entry is the sum of the \((i,j)\)-th entries of \(\mathbb A\) and \(\mathbb B\).
\[
  \mathbb{C}=\begin{bmatrix}
  a_{11}+b_{11} & a_{12}+b_{12} & \ldots & a_{1l}+b_{1l} \\
  a_{21}+b_{21} & a_{22}+b_{22} & \ldots & a_{2l}+b_{2l} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{k1}+b_{k1} & a_{k2}+b_{k2} & \ldots & a_{kl}+b_{kl} \\
  \end{bmatrix}_{k\times l}
  \]
\end{definition}

\begin{definition}[Product of Matrices]
\protect\hypertarget{def:matprod}{}\label{def:matprod}Let \(\mathbb{A},\mathbb{B}\) are matrices both of size \(k\times l\), then the sum \(\mathbb{A}+\mathbb{B}\) is defined as the another matrix \(\mathbb{C}\) size \(k\times l\) such that the \((i,j)\)-th entry is the sum of the \((i,j)\)-th entries of \(\mathbb A\) and \(\mathbb B\).
\[
  \mathbb{C}=\begin{bmatrix}
  a_{11}+b_{11} & a_{12}+b_{12} & \ldots & a_{1l}+b_{1l} \\
  a_{21}+b_{21} & a_{22}+b_{22} & \ldots & a_{2l}+b_{2l} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{k1}+b_{k1} & a_{k2}+b_{k2} & \ldots & a_{kl}+b_{kl} \\
  \end{bmatrix}_{k\times l}
  \]
\end{definition}

\hypertarget{matrix-calculus}{%
\chapter{Matrix Calculus}\label{matrix-calculus}}

  \bibliography{book.bib,packages.bib}

\end{document}
