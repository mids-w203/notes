[["regression.html", "Chapter 4 Regression", " Chapter 4 Regression We write a \\(k\\)-vector (of scalars) as a row \\[ \\v{x}= \\begin{bmatrix} x_1 &amp; x_2 &amp; \\ldots &amp; x_k \\end{bmatrix}. \\] The transpose of \\(\\v{x}\\) as \\[ \\v{x}^T= \\begin{bmatrix} x_1 \\\\ x_2\\\\ \\vdots \\\\ x_k \\end{bmatrix}. \\] We use uppercase letters \\(X,Y,Z,\\ldots\\) to denote random variables. Random vectors are denoted by bold uppercase letters \\(\\v{X},\\v{Y},\\v{Z},\\ldots\\), and written as a row vector. For example, \\[ \\v{X}= \\begin{bmatrix} X_{[1]} &amp; X_{[2]} &amp; \\ldots &amp; X_{[k]} \\end{bmatrix}. \\] In order to distinguish random matrices from vectors, a random matrix is denoted by \\(\\m{X}\\). The expectation of \\(\\v{X}\\) is defined as \\[ \\E{\\v{X}}= \\begin{bmatrix} \\E{X_{[1]}} &amp; \\E{X_{[2]}} &amp; \\ldots &amp; \\E{X_{[k]}} \\end{bmatrix}. \\] The \\(k\\times k\\) covariance matrix of \\(\\v{X}\\) is defined as \\[ \\begin{aligned} \\V{\\v{X}} &amp;=\\E{(\\v{X}-\\E{\\v{X}})^T(\\v{X}-\\E{\\v{X}})} \\\\ &amp;=\\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\ldots &amp; \\sigma_{1k} \\\\ \\sigma_{21} &amp; \\sigma_{2}^2 &amp; \\ldots &amp; \\sigma_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k1} &amp; \\sigma_{k2}^2 &amp; \\ldots &amp; \\sigma_{k}^2 \\\\ \\end{bmatrix}_{k\\times k} \\end{aligned} \\] where \\(\\sigma_j=\\V{X_{[j]}}\\) and \\(\\sigma_{ij}=\\C{X_{[i]},X_{[j]}}\\) for \\(i,j=1,2,\\ldots,k\\) and \\(i\\neq j\\). Theorem 4.1 (Linearity of Exectation) Let \\(\\m{A}_{l\\times k},\\m{B}_{m\\times l}\\) be fixed matrices and \\(\\v{c}\\) a fixed vector of size \\(l\\). If \\(\\v{X}\\) and \\(\\v{Y}\\) are random vectors of size \\(k\\) and \\(m\\), respectively, such that \\(\\E{X}&lt;\\infty,\\E{Y}&lt;\\infty\\), then \\[ \\E{\\m{A}\\v{X}+\\v{Y}\\m{B}+\\v{c}}=\\m{A}\\E{\\v{X}}+\\E{\\v{Y}}\\m{B}+\\v{c}. \\] "],["conditional-expectation-function.html", "4.1 Conditional Expectation Function", " 4.1 Conditional Expectation Function Theorem 4.2 (Characterization of CEF) If  \\(\\E{Y^2}&lt;\\infty\\) and \\(\\v{X}\\) is a random vector such that \\(Y=m(\\v{X})+e\\), then the following statements are equivalent: 1. \\(m(\\v{X})=\\E{Y|\\v{X}}\\), the CEF of \\(Y\\) given \\(\\v{X}\\) 2. \\(\\E{e|\\v{X}}=0\\) "],["best-linear-predictor.html", "4.2 Best Linear Predictor", " 4.2 Best Linear Predictor Let \\(Y\\) be a random variable and \\(\\v{X}\\) be a random vector of \\(k\\) variables. We denote the best linear predictor of \\(Y\\) given \\(\\v{X}\\) by \\(\\mathscr{P}[Y|\\v{X}]\\). It’s also called the linear projection of \\(Y\\) on \\(\\v{X}\\). Theorem 4.3 (Best Linear Predictor) Under the following assumptions \\(\\E{Y^2}&lt;\\infty\\) \\(\\E{||\\bf{X}||^2}&lt;\\infty\\) \\(\\m{Q}_{\\bf{XX}}\\stackrel{\\text{def}}{=}\\E{\\v{X}^T\\v{X}}\\) is positive-definite the best linear predictor exists uniquely, and has the form \\[ \\mathscr{P}[Y|\\v{X}]=\\v{X}\\v{\\beta}, \\] where \\(\\v{\\beta}=\\left(\\E{\\v{X}^T\\v{X}}\\right)^{-1}\\m{E}[\\v{X}^TY]\\) is a column vector. In the following theorem, we show that the BLP error is uncorrelated to the explanatory variables. Theorem 4.4 (Best Linear Predictor Error) If the BLP exists, the linear projection error \\(e=Y-\\mathscr{P}[Y|\\v{X}]\\) follows the following properties: \\(\\m{E}[\\v{X}\\epsilon]=\\v{0}\\) moreover, \\(\\m{E}[\\epsilon]=0\\) if \\(\\v{X}=\\begin{bmatrix}1 &amp; X_{[1]} &amp; \\ldots &amp; X_{[k]} \\end{bmatrix}\\) contains a constant. "]]
