[["index.html", "w203: Statistics for Data Science Cover", " w203: Statistics for Data Science w203 Instructors 2022-02-07 Cover "],["regression.html", "Chapter 1 Regression", " Chapter 1 Regression We write a \\(k\\)-vector (of scalars) as \\[ \\pmb{x}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_k \\end{bmatrix} \\] The transpose of \\(\\pmb{x}\\) as \\[ \\pmb{x}&#39;= \\begin{bmatrix} x_1 &amp; x_2 &amp; \\ldots &amp; x_k \\end{bmatrix}. \\] We use uppercase letters \\(X,Y,Z,\\ldots\\) to denote random variables. Random vectors are denoted by bold uppercase letters \\(\\pmb{X},\\pmb{Y},\\pmb{Z},\\ldots\\), and written as a column vector. For example, \\[ \\pmb{X}= \\begin{bmatrix} X_{1}\\\\ X_{2}\\\\ \\vdots \\\\ X_{k} \\\\ \\end{bmatrix}_{k\\times 1} \\] In order to distinguish random matrices from vectors, a random matrix is denoted by \\(\\mathbb{X}\\). The expectation of \\(\\pmb{X}\\) is defined as \\[ \\mathbb{E}[\\pmb{X}]= \\begin{bmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_k] \\\\ \\end{bmatrix} \\] The \\(k\\times k\\) covariance matrix of \\(\\pmb{X}\\) is \\[\\begin{align} V[\\pmb{X}] &amp;=\\mathbb{E}[(\\pmb{X}-\\mathbb{E}[\\pmb{X}])(\\pmb{X}-\\mathbb{E}[\\pmb{X}])&#39;] \\\\ &amp;=\\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\ldots &amp; \\sigma_{1k} \\\\ \\sigma_{21} &amp; \\sigma_{2}^2 &amp; \\ldots &amp; \\sigma_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k1} &amp; \\sigma_{k2}^2 &amp; \\ldots &amp; \\sigma_{k}^2 \\\\ \\end{bmatrix}_{k\\times k} \\end{align}\\] where \\(\\sigma_j=V[X_j]\\) and \\(\\sigma_{ij}=Cov[X_i,X_j]\\) for \\(i,j=1,2,\\ldots,k\\) and \\(i\\neq j\\). "],["conditional-expectation-function.html", "1.1 Conditional Expectation Function", " 1.1 Conditional Expectation Function Theorem 1.1 If  \\({\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty\\) and \\(\\pmb{X}\\) is a random vector such that \\(Y=m(\\pmb{X})+e\\), then the following statements are equivalent: 1. \\(m(X)=E[Y|\\pmb{X}]\\), the CEF of \\(Y\\) given \\(\\pmb{X}\\) 2. \\({\\mathbb{E}\\left[ e|\\pmb{X} \\right]}=0\\) "],["best-linear-predictor.html", "1.2 Best Linear Predictor", " 1.2 Best Linear Predictor Let \\(Y\\) be a random variable and \\(\\pmb{X}\\) be a random vector. We denote the best linear predictor of \\(Y\\) given \\(\\pmb{X}\\) by \\(\\mathscr{P}[Y|\\pmb{X}]\\). It’s also called the linear projection of \\(Y\\) on \\(\\pmb{X}\\). Theorem 1.2 (Best Linear Predictor) Under the following assumptions \\(\\mathbb{E}\\left[Y^2\\right]&lt;\\infty\\) \\(\\mathbb{E}||\\pmb{X}||^2&lt;\\infty\\) \\(\\mathbb{Q}_{\\pmb{XX}}:=\\mathbb{E}\\left[\\pmb{X}\\pmb{X}&#39;\\right]\\) is positive-definite the best linear predictor exists uniquely, and has the form \\[ \\mathscr{P}[Y|\\pmb{X}]=\\pmb{X}&#39;\\beta, \\] where \\(\\beta=\\left(\\mathbb{E}[\\pmb{X}\\pmb{X}&#39;]\\right)^{-1}\\mathbb{E}[\\pmb{X}Y]\\). Theorem 1.3 (Best Linear Predictor Error) If the BLP exists, the linear projection error \\(e=Y-\\mathscr{P}[Y|\\pmb{X}]\\) follows the following properties: \\(\\mathbb{E}[\\pmb{X}e]=\\pmb{0}\\) \\(\\mathbb{E}[e]=0\\) if \\(\\pmb{X}&#39;=\\begin{bmatrix}1 &amp; X_1 &amp; \\ldots &amp; X_k \\end{bmatrix}\\). "],["ordinary-least-squares.html", "Chapter 2 Ordinary Least Squares", " Chapter 2 Ordinary Least Squares Let \\(Y\\) be our outcome random variable and \\[ \\pmb{X}=\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix}_{(k+1)\\times 1} \\] be our predictor vector containing \\(k\\) predictors and a constant. We denote the joint distribution of \\((Y,\\pmb{X})\\) by \\(F(y,\\pmb{x})\\), i.e., \\[ F(y,\\pmb{x})=\\mathbb{P}(Y\\leq y, \\pmb{X}\\leq\\pmb{x}) =\\mathbb{P}(Y\\leq y,X_1\\leq x_1,\\ldots,X_k\\leq x_k). \\] The dataset or sample is a collection of observations \\(\\{(Y_i,\\pmb{X}_i): i=1,2,\\ldots,n\\}\\). We assume that each observation \\((Y_i,\\pmb{X}_i)\\) is a random vector drawn from the common distribution or population \\(F\\). "],["matrix-formulation.html", "2.1 Matrix Formulation", " 2.1 Matrix Formulation For a given vector of (unknown) coefficients \\(\\pmb{\\beta}=\\begin{bmatrix}\\beta_0 &amp; \\beta_1 &amp; \\ldots &amp; \\beta_k\\end{bmatrix}&#39;\\in\\mathbb{R}^{k+1}\\), we define the following cost function: \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2. \\] The cost function \\(\\widehat{S}({\\pmb{\\beta}})\\) can also be thought of as the average sum of residuals. In fact, \\(\\widehat{S}({\\pmb{\\beta}})\\) is the moment (plug-in) estimator of the mean squared error, \\[ S(\\pmb{\\beta})={\\mathbb{E}\\left[ (Y-\\pmb{X}&#39;\\pmb{\\beta})^2 \\right]}. \\] We now minimize \\(\\widehat{S}({\\pmb{\\beta}})\\) over all possible choices of \\(\\pmb{\\beta}\\in\\mathbb{R}^{k+1}\\). When the minimizer exists and is unique, we call it the least squares estimator, denoted \\(\\widehat{\\pmb{\\beta}}\\). Definition 2.1 ((Ordinary) Least Squares Estimator) The least square estimator is \\[ \\widehat{\\pmb{\\beta}} =\\underset{\\pmb{\\beta}\\in\\mathbb{R}^{k+1}}{\\arg\\min} \\ \\widehat{S}(\\pmb{\\beta}), \\] provided it exists uniquely. "],["solution-of-ols.html", "2.2 Solution of OLS", " 2.2 Solution of OLS We rewrite the cost function as \\[ \\widehat{S}(\\pmb{\\beta})=\\frac{1}{n}SSE(\\pmb{\\beta}), \\] where \\(SSE(\\pmb{\\beta}):=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2\\). We now express \\(SSE(\\pmb{\\beta})\\) as a quadratic function of \\(\\pmb{\\beta}&#39;\\). \\[\\begin{align} SSE &amp;=\\sum\\limits_{i=1}^n(Y_i-\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{X_i}&#39;\\pmb{\\beta}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i(\\pmb{\\beta}&#39;\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{X_i}&#39;\\pmb{\\beta})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n \\pmb{\\beta}&#39;(Y_i\\pmb{X_i}) + \\sum\\limits_{i=1}^n (\\pmb{\\beta}&#39;\\pmb{X_i})(\\pmb{X_i}&#39;\\pmb{\\beta}) \\\\ &amp;=\\left(\\sum\\limits_{i=1}^n Y_i^2\\right) - 2\\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + \\pmb{\\beta}&#39;\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta} \\end{align}\\] Taking partial derivative w.r.t. \\(\\beta_j\\), we get \\[ \\frac{\\partial}{\\partial\\beta_j}SSE(\\pmb{\\beta})=-2\\left[\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right]_j + 2\\left[\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}\\right]_j. \\] Therefore, \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta}) =-2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\pmb{\\beta}. \\] In order to miniminize \\(SSE(\\pmb{\\beta})\\), a necessary condition for \\(\\widehat{\\pmb{\\beta}}\\) is \\[ \\frac{\\partial}{\\partial\\pmb{\\beta}}SSE(\\pmb{\\beta})\\bigg|_{\\pmb{\\beta} =\\widehat{\\pmb{\\beta}}}=\\pmb{0}, \\] i.e., \\[ -2\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) + 2\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} =\\pmb{0} \\] So, \\[\\begin{equation} \\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right) =\\left(\\sum\\limits_{i=1}^n \\pmb{X_i}\\pmb{X_i}&#39;\\right)\\widehat{\\pmb{\\beta}} \\tag{2.1} \\end{equation}\\] Both the left and right hand side of the above equation are \\(k+1\\) vectors. So, we have a system of \\((k+1)\\) linear equations with \\((k+1)\\) unknowns—the elements of \\(\\pmb{\\beta}\\). Let us define \\[ \\widehat{\\mathbb{Q}}_{\\pmb{XX}} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}\\pmb{X_i}&#39;\\right) \\mbox{ and } \\widehat{\\mathbb{Q}}_{\\pmb{X}Y} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n\\pmb{X_i}Y_i\\right). \\] Rewriting (2.1), we get \\[\\begin{equation} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}=\\widehat{\\mathbb{Q}}_{\\pmb{XX}} \\widehat{\\pmb{\\beta}}. \\tag{2.2} \\end{equation}\\] Equation (2.2) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular. In that case, we can solve for \\(\\widehat{\\pmb{\\beta}}\\) to get, \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] To verify that the above choice minimizes \\(SSE(\\pmb{\\beta})\\), one can consider the second-order moment conditions. \\[ \\frac{\\partial^2}{\\partial\\pmb{\\beta}\\partial\\pmb{\\beta}&#39;}SSE(\\pmb{\\beta}) =2\\widehat{\\mathbb{Q}}_{\\pmb{XX}}. \\] If \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, it is also positive-definite. So, we have actually proved the following theorem. Theorem 2.1 If  \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, then the least squares estimator is unique, and is given by \\[ \\widehat{\\pmb{\\beta}}=\\left[\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{\\pmb{X}Y}. \\] "],["errors-and-residuals.html", "2.3 Errors and Residuals", " 2.3 Errors and Residuals We first define the fitted value as \\[ \\widehat{Y}_i=\\pmb{X}_i&#39;\\widehat{\\pmb{\\beta}}\\mbox{ for } i=1,2,\\ldots,n. \\] For the least squares estimators, we define the errors and residuals in the following way: \\[ e_i=Y_i-\\pmb{X}&#39;\\pmb{\\beta}, \\mbox{ and } \\widehat{e}_i=Y_i-\\widehat{Y}_i. \\] Theorem 2.2 (Least Squares Error) If  \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}\\) is non-singular, then 1. \\(\\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{e}_i=\\pmb{0}\\) 2. \\(\\sum\\limits_{i=1}^n\\widehat{e}_i=0\\) Proof. \\[\\begin{align} \\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{e}_i &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_i(Y_i-\\widehat{Y}_i) \\\\ &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i-\\sum\\limits_{i=1}^n\\pmb{X}_i\\widehat{Y}_i \\\\ &amp;=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i-\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\pmb{\\widehat{\\beta}} \\\\ &amp;=\\widehat{Q}_{\\pmb{X}Y}-\\widehat{Q}_{\\pmb{XX}}\\pmb{\\widehat{\\beta}} \\\\ &amp;=\\widehat{Q}_{\\pmb{X}Y}-\\widehat{Q}_{\\pmb{XX}} \\left( \\widehat{Q}_{\\pmb{XX}}^{-1} \\widehat{Q}_{\\pmb{X}Y} \\right) \\\\ &amp;=\\pmb{0} \\end{align}\\] From the first row of (1) we get \\[ \\sum\\limits_{i=1}^n X_{i1}\\widehat{e}_i=0. \\] Since \\(X_{i1}=1\\) for all \\(i\\), we have that \\[ \\sum\\limits_{i=1}^n\\widehat{e}_i=0. \\] Hence the result. "],["model-in-matrix-notation.html", "2.4 Model in Matrix Notation", " 2.4 Model in Matrix Notation Taking the definition of errors from the last section, we can write down a system of \\(n\\) linear equations: \\[\\begin{align} Y_1 &amp;= \\pmb{X_1}&#39;\\pmb{\\beta} + e_1 \\\\ Y_2 &amp;= \\pmb{X_2}&#39;\\pmb{\\beta} + e_2 \\\\ &amp; \\vdots \\\\ Y_n &amp;= \\pmb{X_1}&#39;\\pmb{\\beta} + e_n \\end{align}\\] Define \\[ \\pmb{Y}=\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}_{n\\times1},\\ \\mathbb{X}=\\begin{bmatrix} \\pmb{X}_1 \\\\ \\pmb{X}_2 \\\\ \\vdots \\\\ \\pmb{X}_n \\end{bmatrix}_{n\\times(k+1)}, \\mbox{ and } \\pmb{e}=\\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}_{n\\times1}. \\] We can now rewrite the system as the following: \\[ \\pmb{Y}=\\mathbb{X}\\pmb{\\beta}+\\pmb{e}. \\] Note that \\[ \\mathbb{X}=\\begin{bmatrix} 1 &amp; X_{11} &amp; X_{12} &amp; \\ldots &amp; X_{1k} \\\\ 1 &amp; X_{21} &amp; X_{22} &amp; \\ldots &amp; X_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{n1} &amp; X_{n2} &amp; \\ldots &amp; X_{nk} \\end{bmatrix} \\] We also note that \\[ \\widehat{Q}_{\\pmb{XX}}=\\sum\\limits_{i=1}^n\\pmb{X}_i&#39;\\pmb{X}_i= \\mathbb{X}&#39;\\mathbb{X}, \\] and \\[ \\widehat{Q}_{\\pmb{X}Y}=\\sum\\limits_{i=1}^n\\pmb{X}_iY_i= \\mathbb{X}&#39;\\pmb{Y}. \\] So, we have write the least squares estimator as \\[ \\widehat{\\pmb{\\beta}}=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}\\pmb{Y}. \\] Similarly, the residual vector is \\[ \\widehat{\\pmb{e}}=\\pmb{Y}-\\mathbb{X}\\widehat{\\pmb{\\beta}}. \\] As a consequence, we can write \\[ \\mathbb{X}&#39;\\widehat{\\pmb{e}}=\\pmb{0}. \\] "],["linear-conditional-expectation-function.html", "Chapter 3 Linear Conditional Expectation Function ", " Chapter 3 Linear Conditional Expectation Function "],["variance-of-error.html", "3.1 Variance of Error", " 3.1 Variance of Error We first compute the (unconditional) variance of the error vector \\(\\pmb{e}\\). The covariance matrix \\[ \\mathbb{V}[\\pmb{e}]={\\mathbb{E}\\left[ \\pmb{e}\\pmb{e}&#39; \\right]}-{\\mathbb{E}\\left[ \\pmb{e} \\right]}{\\mathbb{E}\\left[ \\pmb{e}&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{e}\\pmb{e}&#39; \\right]}\\stackrel{\\text{def}}{=}\\mathbb{D}. \\] For \\(i\\neq j\\), the errors \\(e_i\\),\\(e_j\\) are independent. As a result, \\({\\mathbb{E}\\left[ e_ie_j \\right]}={\\mathbb{E}\\left[ e_i \\right]}{\\mathbb{E}\\left[ e_j \\right]}=0\\). So, \\(\\mathbb{D}\\) is a diagonal matrix with the \\(i\\)-th diagonal element \\(\\sigma_i^2\\): \\[ \\mathbb{D}=\\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\sigma_n^2 \\end{bmatrix}. \\] "],["variance-of-ols-estimators.html", "3.2 Variance of OLS Estimators", " 3.2 Variance of OLS Estimators "],["large-sample-regression.html", "Chapter 4 Large-Sample Regression", " Chapter 4 Large-Sample Regression We assume that the best linear predictor, \\(\\mathscr{P}[Y|\\pmb{X}]\\), of \\(Y\\) given \\(\\pmb{X}\\) is \\(\\pmb{X}&#39;\\pmb{\\beta}\\). If we write \\[ Y=\\pmb{X}&#39;\\pmb{\\beta}+e. \\] we have from Theorem 1.3 \\[{\\mathbb{E}\\left[ e \\right]}=0,\\mbox{ and }{\\mathbb{E}\\left[ \\pmb{X}e \\right]}=\\pmb{0}.\\] We also assume that the dataset \\(\\{(Y_i,\\pmb{X}_i)\\}\\) are taken i.i.d. from the joint distribution of \\((Y,\\pmb{X})\\). For each \\(i\\), we can write \\[ Y_i=\\pmb{X_i}&#39;\\pmb{\\beta}+e_i. \\] In matrix notation, we can write \\[ \\pmb{Y}=\\mathbb{X}&#39;\\pmb{\\beta}+\\pmb{e}. \\] Then \\[{\\mathbb{E}\\left[ \\pmb{e} \\right]}=\\pmb{0}\\]. "],["consistency-of-ols-estimators.html", "4.1 Consistency of OLS Estimators", " 4.1 Consistency of OLS Estimators "],["asymptotic-normality.html", "4.2 Asymptotic Normality", " 4.2 Asymptotic Normality We start by revealing an alternative expression for the OLS estimators \\(\\widehat{\\pmb{\\beta}}\\) using matrix notation. \\[\\begin{align} \\widehat{\\pmb{\\beta}} &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{Y} \\\\ &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;(\\mathbb{X}\\pmb{\\beta}+\\pmb{e}) \\\\ &amp;=\\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}(\\mathbb{X}&#39;\\mathbb{X})\\pmb{\\beta}+ \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\\\ &amp;=\\pmb{\\beta} + \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\end{align}\\] So, \\[\\begin{equation} \\widehat{\\pmb{\\beta}}-\\pmb{\\beta} = \\left[\\mathbb{X}&#39;\\mathbb{X}\\right]^{-1}\\mathbb{X}&#39;\\pmb{e} \\tag{4.1} \\end{equation}\\] We can then multiply by \\(\\sqrt{n}\\) both sides of Equation (4.1) to get \\[\\begin{align} \\sqrt{n}\\left(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta}\\right) &amp;=\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39; \\right)^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\right) \\\\ &amp;=\\widehat{\\mathbb Q}_{\\pmb{XX}}^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\right) \\end{align}\\] From the consistency of OLS estimators, we already have \\[ \\widehat{\\mathbb Q}_{\\pmb{XX}}\\xrightarrow[p]{\\quad\\quad}\\mathbb{Q}_{\\pmb{XX}}\\] Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression. We first note (from i.i.d. and Theorem 1.3) that \\[ {\\mathbb{E}\\left[ \\pmb{X}_ie_i \\right]}={\\mathbb{E}\\left[ \\pmb{X}e \\right]}=\\pmb{0}. \\] Let us compute the covariance matrix of \\(\\pmb{X}_ie_i\\). Since the expectation vector is zero, we have \\[ \\mathbb{V}[\\pmb{X}_ie_i]={\\mathbb{E}\\left[ \\pmb{X}_ie_i(\\pmb{X}_ie_i)&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{X}e(\\pmb{X}e)&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39;e^2 \\right]}\\stackrel{\\text{def}}{=}\\mathbb{A}. \\] As any function of \\(\\{(Y_i,\\pmb{X}_i)\\}\\)’s are independent, \\(\\{\\pmb{X}_ie_i\\}\\)’s are independent. By the (multivariate) Central Limit Theorem, as \\(n\\to\\infty\\) \\[ \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n\\pmb{X}_ie_i \\xrightarrow[d]{\\quad\\quad}\\mathcal{N}(\\pmb{0},\\mathbb{A}). \\] There is a small technicality here, we must have \\(\\mathbb{A}&lt;\\infty\\). This can be imposed by a stronger regularity condition on the moments, e.g., \\({\\mathbb{E}\\left[ Y^4 \\right]},{\\mathbb{E}\\left[ ||\\pmb{X}||^4 \\right]}&lt;\\infty\\). Putting everything together, we conclude \\[ \\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\xrightarrow[d]{\\quad\\quad} \\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathcal{N}(\\pmb{0},\\mathbb{A}) =\\mathcal{N}\\left(0,\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\right) \\] Theorem 4.1 (Asymptotic Distribution of OLS Estimators) We assume the following: 1. The observations \\(\\{(Y_i,\\pmb{X}_i)\\}_{i=1}^n\\) are i.i.d from the joint distribution of \\((Y,\\pmb{X})\\) 2. \\({\\mathbb{E}\\left[ Y^4 \\right]}&lt;\\infty\\) 3. \\({\\mathbb{E}\\left[ ||\\pmb{X}||^4 \\right]}&lt;\\infty\\) 4. \\(\\mathbb{Q}_{\\pmb{XX}}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39; \\right]}\\) is positive-definite. Under these assumptions, as \\(n\\to\\infty\\) \\[ \\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\xrightarrow[d]{\\quad\\quad} \\mathcal{N}\\left(\\pmb{0},\\mathbb{V}_{\\pmb{\\beta}}\\right), \\] where \\[\\mathbb{V}_{\\pmb{\\beta}}\\stackrel{\\text{def}}{=}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}\\] and \\(\\mathbb{Q}_{\\pmb{XX}}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39; \\right]}\\), \\(\\mathbb{A}={\\mathbb{E}\\left[ \\pmb{X}\\pmb{X}&#39;e^2 \\right]}\\). The covariance matrix \\(\\mathbb{V}_{\\pmb{\\beta}}\\) is called the asymptotic variance matrix of \\(\\widehat{\\pmb{\\beta}}\\). The matrix is sometimes referred to as the sandwich form. "],["covariance-matrix-estimation.html", "4.3 Covariance Matrix Estimation", " 4.3 Covariance Matrix Estimation We now turn our attention to the estimation of the sandwich matrix using a finite sample. 4.3.1 Heteroskedastic Variance Theorem 4.1 presented the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta})\\) is \\[\\mathbb{V}_{\\pmb{\\beta}} =\\mathbb{Q}_{\\pmb{XX}}^{-1}\\mathbb{A}\\mathbb{Q}_{\\pmb{XX}}^{-1}.\\] Without imposing any homoskedasticity condition, we estimate \\(\\mathbb{V}_{\\pmb{\\beta}}\\) using a plug-in estimator. We have already seen that \\(\\widehat{\\mathbb{Q}}_{\\pmb{XX}}=\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\) is a natural estimator for \\(\\mathbb{Q}_{\\pmb{XX}}\\). For \\(\\mathbb{A}\\), we use the moment estimator \\[ \\widehat{\\mathbb{A}}=\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\widehat{e}_i^2, \\] where \\(\\widehat{e}_i=(Y_i-\\pmb{X}_i&#39;\\widehat{\\pmb{\\beta}})\\) is the \\(i\\)-th residual. As it turns out, \\(\\widehat{\\mathbb{A}}\\) is a consistent estimator for \\(\\mathbb{A}\\). As a result, we get the following plug-in estimator for \\(\\mathbb{V}_{\\pmb{\\beta}}\\): \\[ \\widehat{\\mathbb{V}}_{\\pmb{\\beta}}^{\\text{HC0}}= \\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1}\\widehat{\\mathbb{A}}\\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1} \\] The estimator is also consistent. For a proof, see Hensen 2013. As a consequence, we can get the following estimator for the variance, \\(\\mathbb{V}_{\\widehat{\\pmb{\\beta}}}\\), of \\(\\widehat{\\pmb{\\beta}}\\) in the heteroskedastic case. \\[\\begin{align} \\widehat{\\mathbb{V}}^{\\text{HC0}}_{\\widehat{\\pmb{\\beta}}} &amp;=\\frac{1}{n}\\widehat{\\mathbb{V}}_{\\pmb{\\beta}}^{\\text{HC0}} \\\\ &amp;=\\frac{1}{n}\\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1}\\widehat{\\mathbb{A}}\\widehat{\\mathbb{Q}}_{\\pmb{XX}}^{-1} \\\\ &amp;=\\frac{1}{n}\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\widehat{e}_i^2\\right) \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n\\pmb{X}_i\\pmb{X}_i&#39;\\right)^{-1} \\\\ &amp;=\\left(\\mathbb{X}\\mathbb{X}&#39;\\right)^{-1} \\mathbb{X}\\mathbb{D}\\mathbb{X}&#39; \\left(\\mathbb{X}\\mathbb{X}&#39;\\right)^{-1} \\end{align}\\] where \\(\\mathbb{D}\\) is an \\(n\\times n\\) diagonal matrix with diagonal entries \\(\\widehat{e}_1^2,\\widehat{e}_2^2,\\ldots,\\widehat{e}_n^2\\). The estimator \\(\\widehat{\\mathbb{V}}^{\\text{HC0}}_{\\widehat{\\pmb{\\beta}}}\\) is referred to as the robust error variance estimator for the OLS coefficients \\(\\widehat{\\pmb{\\beta}}\\). 4.3.2 Homeskedastic Variance "],["references.html", "References", " References "],["proofs.html", "A Proofs", " A Proofs "]]
