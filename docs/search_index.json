[["index.html", "w203: Statistics for Data Science Cover", " w203: Statistics for Data Science w203 Instructors 2022-02-10 Cover "],["probability-spaces.html", "Chapter 1 Probability Spaces ", " Chapter 1 Probability Spaces "],["kolmogorovs-axioms.html", "1.1 Kolmogorov’s Axioms", " 1.1 Kolmogorov’s Axioms "],["conditional-probability.html", "1.2 Conditional Probability", " 1.2 Conditional Probability "],["random-variables.html", "Chapter 2 Random Variables", " Chapter 2 Random Variables "],["hypothesis-testing.html", "Chapter 3 Hypothesis Testing", " Chapter 3 Hypothesis Testing "],["regression.html", "Chapter 4 Regression", " Chapter 4 Regression We write a \\(k\\)-vector (of scalars) as a row \\[ {\\boldsymbol{x}}= \\begin{bmatrix} x_1 &amp; x_2 &amp; \\ldots &amp; x_k \\end{bmatrix}. \\] The transpose of \\({\\boldsymbol{x}}\\) as \\[ {\\boldsymbol{x}}^T= \\begin{bmatrix} x_1 \\\\ x_2\\\\ \\vdots \\\\ x_k \\end{bmatrix}. \\] We use uppercase letters \\(X,Y,Z,\\ldots\\) to denote random variables. Random vectors are denoted by bold uppercase letters \\({\\boldsymbol{X}},{\\boldsymbol{Y}},{\\boldsymbol{Z}},\\ldots\\), and written as a row vector. For example, \\[ {\\boldsymbol{X}}= \\begin{bmatrix} X_{[1]} &amp; X_{[2]} &amp; \\ldots &amp; X_{[k]} \\end{bmatrix}. \\] In order to distinguish random matrices from vectors, a random matrix is denoted by \\({\\mathbb{X}}\\). The expectation of \\({\\boldsymbol{X}}\\) is defined as \\[ {\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}= \\begin{bmatrix} {\\mathbb{E}\\left[ X_{[1]} \\right]} &amp; {\\mathbb{E}\\left[ X_{[2]} \\right]} &amp; \\ldots &amp; {\\mathbb{E}\\left[ X_{[k]} \\right]} \\end{bmatrix}. \\] The \\(k\\times k\\) covariance matrix of \\({\\boldsymbol{X}}\\) is defined as \\[ \\begin{aligned} {\\mathbb{V}\\left[ {\\boldsymbol{X}} \\right]} &amp;={\\mathbb{E}\\left[ ({\\boldsymbol{X}}-{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]})^T({\\boldsymbol{X}}-{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}) \\right]} \\\\ &amp;=\\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\ldots &amp; \\sigma_{1k} \\\\ \\sigma_{21} &amp; \\sigma_{2}^2 &amp; \\ldots &amp; \\sigma_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{k1} &amp; \\sigma_{k2}^2 &amp; \\ldots &amp; \\sigma_{k}^2 \\\\ \\end{bmatrix}_{k\\times k} \\end{aligned} \\] where \\(\\sigma_j={\\mathbb{V}\\left[ X_{[j]} \\right]}\\) and \\(\\sigma_{ij}={\\text{Cov}\\left[ X_{[i]},X_{[j]} \\right]}\\) for \\(i,j=1,2,\\ldots,k\\) and \\(i\\neq j\\). Theorem 4.1 (Linearity of Exectation) Let \\({\\mathbb{A}}_{l\\times k},{\\mathbb{B}}_{m\\times l}\\) be fixed matrices and \\({\\boldsymbol{c}}\\) a fixed vector of size \\(l\\). If \\({\\boldsymbol{X}}\\) and \\({\\boldsymbol{Y}}\\) are random vectors of size \\(k\\) and \\(m\\), respectively, such that \\({\\mathbb{E}\\left[ X \\right]}&lt;\\infty,{\\mathbb{E}\\left[ Y \\right]}&lt;\\infty\\), then \\[ {\\mathbb{E}\\left[ {\\mathbb{A}}{\\boldsymbol{X}}+{\\boldsymbol{Y}}{\\mathbb{B}}+{\\boldsymbol{c}} \\right]}={\\mathbb{A}}{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}+{\\mathbb{E}\\left[ {\\boldsymbol{Y}} \\right]}{\\mathbb{B}}+{\\boldsymbol{c}}. \\] "],["conditional-expectation-function.html", "4.1 Conditional Expectation Function", " 4.1 Conditional Expectation Function Theorem 4.2 (Characterization of CEF) If  \\({\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty\\) and \\({\\boldsymbol{X}}\\) is a random vector such that \\(Y=m({\\boldsymbol{X}})+e\\), then the following statements are equivalent: 1. \\(m({\\boldsymbol{X}})={\\mathbb{E}\\left[ Y|{\\boldsymbol{X}} \\right]}\\), the CEF of \\(Y\\) given \\({\\boldsymbol{X}}\\) 2. \\({\\mathbb{E}\\left[ e|{\\boldsymbol{X}} \\right]}=0\\) "],["best-linear-predictor.html", "4.2 Best Linear Predictor", " 4.2 Best Linear Predictor Let \\(Y\\) be a random variable and \\({\\boldsymbol{X}}\\) be a random vector of \\(k\\) variables. We denote the best linear predictor of \\(Y\\) given \\({\\boldsymbol{X}}\\) by \\(\\mathscr{P}[Y|{\\boldsymbol{X}}]\\). It’s also called the linear projection of \\(Y\\) on \\({\\boldsymbol{X}}\\). Theorem 4.3 (Best Linear Predictor) Under the following assumptions \\({\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty\\) \\({\\mathbb{E}\\left[ ||\\bf{X}||^2 \\right]}&lt;\\infty\\) \\({\\mathbb{Q}}_{\\bf{XX}}\\stackrel{\\text{def}}{=}{\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}} \\right]}\\) is positive-definite the best linear predictor exists uniquely, and has the form \\[ \\mathscr{P}[Y|{\\boldsymbol{X}}]={\\boldsymbol{X}}{\\boldsymbol{\\beta}}, \\] where \\({\\boldsymbol{\\beta}}=\\left({\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}} \\right]}\\right)^{-1}{\\mathbb{E}}[{\\boldsymbol{X}}^TY]\\) is a column vector. In the following theorem, we show that the BLP error is uncorrelated to the explanatory variables. Theorem 4.4 (Best Linear Predictor Error) If the BLP exists, the linear projection error \\(\\varepsilon=Y-\\mathscr{P}[Y|{\\boldsymbol{X}}]\\) follows the following properties: \\({\\mathbb{E}}[{\\boldsymbol{X}}\\varepsilon]={\\boldsymbol{0}}\\) moreover, \\({\\mathbb{E}}[\\varepsilon]=0\\) if \\({\\boldsymbol{X}}=\\begin{bmatrix}1 &amp; X_{[1]} &amp; \\ldots &amp; X_{[k]} \\end{bmatrix}\\) contains a constant. "],["ordinary-least-squares.html", "Chapter 5 Ordinary Least Squares", " Chapter 5 Ordinary Least Squares Let \\(Y\\) be our outcome random variable and \\[ {\\boldsymbol{X}}=\\begin{bmatrix} 1 &amp; X_{[1]} &amp; X_{[2]} &amp; \\ldots &amp; X_{[k]} \\end{bmatrix} \\] be our predictor (or explanatory) vector containing \\(k\\) predictors and a constant. We denote the joint distribution of \\((Y,{\\boldsymbol{X}})\\) by \\(F(y,{\\boldsymbol{x}})\\), i.e., \\[ F(y,{\\boldsymbol{x}})={\\mathbb{P}\\left(Y\\leq y, {\\boldsymbol{X}}\\leq{\\boldsymbol{x}}\\right)} ={\\mathbb{P}\\left(Y\\leq y,X_1\\leq x_1,\\ldots,X_k\\leq x_k\\right)}. \\] The dataset or sample is a collection of observations \\(\\{(Y_i,{\\boldsymbol{X}}_i): i=1,2,\\ldots,n\\}\\). We assume that each observation \\((Y_i,{\\boldsymbol{X}}_i)\\) is a random (row) vector drawn from the common distribution, sometimes referred to as the population, \\(F\\). For a given vector of (unknown) coefficients \\({\\boldsymbol{\\beta}}=\\begin{bmatrix}\\beta_0 &amp; \\beta_1 &amp; \\ldots &amp; \\beta_k\\end{bmatrix}^T\\in\\mathbb{R}^{k+1}\\), we define the following cost function: \\[ \\widehat{S}({\\boldsymbol{\\beta}})=\\frac{1}{n}\\sum\\limits_{i=1}^n(Y_i-{\\boldsymbol{X_i}}{\\boldsymbol{\\beta}})^2. \\] The cost function \\(\\widehat{S}({{\\boldsymbol{\\beta}}})\\) can also be thought of as the average sum of residuals. In fact, \\(\\widehat{S}({{\\boldsymbol{\\beta}}})\\) is the moment (plug-in) estimator of the mean squared error, \\[ S({\\boldsymbol{\\beta}})={\\mathbb{E}\\left[ (Y-{\\boldsymbol{X}}{\\boldsymbol{\\beta}})^2 \\right]}. \\] We now minimize \\(\\widehat{S}({{\\boldsymbol{\\beta}}})\\) over all possible choices of \\({\\boldsymbol{\\beta}}\\in\\mathbb{R}^{k+1}\\). When the minimizer exists and is unique, we call it the least squares estimator, denoted \\(\\widehat{{\\boldsymbol{\\beta}}}\\). Definition 5.1 ((Ordinary) Least Squares Estimator) The least square estimator is \\[ \\widehat{{\\boldsymbol{\\beta}}} =\\underset{{\\boldsymbol{\\beta}}\\in\\mathbb{R}^{k+1}}{\\arg\\min} \\ \\widehat{S}({\\boldsymbol{\\beta}}), \\] provided it exists uniquely. "],["solution-of-ols.html", "5.1 Solution of OLS", " 5.1 Solution of OLS We rewrite the cost function as \\[ \\widehat{S}({\\boldsymbol{\\beta}})=\\frac{1}{n}SSE({\\boldsymbol{\\beta}}), \\] where \\(SSE({\\boldsymbol{\\beta}})\\stackrel{\\text{def}}{=}\\sum\\limits_{i=1}^n(Y_i-{\\boldsymbol{X_i}}{\\boldsymbol{\\beta}})^2\\). We now express \\(SSE({\\boldsymbol{\\beta}})\\) as a quadratic function of \\({\\boldsymbol{\\beta}}\\). \\[ \\begin{aligned} SSE &amp;=\\sum\\limits_{i=1}^n(Y_i-{\\boldsymbol{X_i}}{\\boldsymbol{\\beta}})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i({\\boldsymbol{X_i}}{\\boldsymbol{\\beta}}) + \\sum\\limits_{i=1}^n ({\\boldsymbol{X_i}}{\\boldsymbol{\\beta}})^2 \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n Y_i({\\boldsymbol{\\beta}}^T{\\boldsymbol{X_i}}^T) + \\sum\\limits_{i=1}^n ({\\boldsymbol{X_i}}{\\boldsymbol{\\beta}})({\\boldsymbol{X_i}}{\\boldsymbol{\\beta}}) \\\\ &amp;=\\sum\\limits_{i=1}^n Y_i^2 - 2\\sum\\limits_{i=1}^n {\\boldsymbol{\\beta}}^T(Y_i{\\boldsymbol{X_i}}^T) + \\sum\\limits_{i=1}^n ({\\boldsymbol{\\beta}}^T{\\boldsymbol{X_i}}^T)({\\boldsymbol{X_i}}{\\boldsymbol{\\beta}}) \\\\ &amp;=\\left(\\sum\\limits_{i=1}^n Y_i^2\\right) - 2{\\boldsymbol{\\beta}}^T\\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right) + {\\boldsymbol{\\beta}}^T\\left(\\sum\\limits_{i=1}^n {\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right){\\boldsymbol{\\beta}} \\end{aligned} \\] Taking partial derivative w.r.t. \\(\\beta_j\\), we get \\[ \\frac{\\partial}{\\partial\\beta_j}SSE({\\boldsymbol{\\beta}})=-2\\left[\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right]_j + 2\\left[\\left(\\sum\\limits_{i=1}^n {\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right){\\boldsymbol{\\beta}}\\right]_j. \\] Therefore, \\[ \\frac{\\partial}{\\partial{\\boldsymbol{\\beta}}}SSE({\\boldsymbol{\\beta}}) =-2\\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right) + 2\\left(\\sum\\limits_{i=1}^n {\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right){\\boldsymbol{\\beta}}. \\] In order to miniminize \\(SSE({\\boldsymbol{\\beta}})\\), a necessary condition for \\(\\widehat{{\\boldsymbol{\\beta}}}\\) is \\[ \\frac{\\partial}{\\partial{\\boldsymbol{\\beta}}}SSE({\\boldsymbol{\\beta}})\\bigg|_{{\\boldsymbol{\\beta}} =\\widehat{{\\boldsymbol{\\beta}}}}={\\boldsymbol{0}}, \\] i.e., \\[ -2\\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right) + 2\\left(\\sum\\limits_{i=1}^n {\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right)\\widehat{{\\boldsymbol{\\beta}}} ={\\boldsymbol{0}} \\] So, \\[\\begin{equation} \\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right) =\\left(\\sum\\limits_{i=1}^n {\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right)\\widehat{{\\boldsymbol{\\beta}}} \\tag{5.1} \\end{equation}\\] Both the left and right hand side of the above equation are \\(k+1\\) vectors. So, we have a system of \\((k+1)\\) linear equations with \\((k+1)\\) unknowns—the elements of \\({\\boldsymbol{\\beta}}\\). Let us define \\[ \\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^T{\\boldsymbol{X_i}}\\right) \\mbox{ and } \\widehat{\\mathbb{Q}}_{{\\boldsymbol{X}}Y} =\\frac{1}{n}\\left(\\sum\\limits_{i=1}^n{\\boldsymbol{X_i}}^TY_i\\right). \\] Rewriting (5.1), we get \\[\\begin{equation} \\widehat{\\mathbb{Q}}_{{\\boldsymbol{X}}Y}=\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}} \\widehat{{\\boldsymbol{\\beta}}}. \\tag{5.2} \\end{equation}\\] Equation (5.2) is sometimes referred to as the first-order moment condition. For the uniqueness of solution, we require that \\(\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\) is non-singular. In that case, we can solve for \\(\\widehat{{\\boldsymbol{\\beta}}}\\) to get, \\[ \\widehat{{\\boldsymbol{\\beta}}}=\\left[\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{{\\boldsymbol{X}}Y}. \\] To verify that the above choice minimizes \\(SSE({\\boldsymbol{\\beta}})\\), one can consider the second-order moment conditions. \\[ \\frac{\\partial^2}{\\partial{\\boldsymbol{\\beta}}\\partial{\\boldsymbol{\\beta}}^T}SSE({\\boldsymbol{\\beta}}) =2\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}. \\] If \\(\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\) is non-singular, it is also positive-definite. So, we have actually proved the following theorem. Theorem 5.1 If  \\(\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\) is non-singular, then the least squares estimator is unique, and is given by \\[ \\widehat{{\\boldsymbol{\\beta}}}=\\left[\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\right]^{-1} \\widehat{\\mathbb{Q}}_{{\\boldsymbol{X}}Y}. \\] "],["errors-and-residuals.html", "5.2 Errors and Residuals", " 5.2 Errors and Residuals Recall that \\({\\boldsymbol{\\beta}}\\) denotes the coefficients of the best linear predictor 4.3. We first define the fitted value as \\[ \\widehat{Y}_i={\\boldsymbol{X}}_i\\widehat{{\\boldsymbol{\\beta}}}\\mbox{ for } i=1,2,\\ldots,n. \\] For the least squares estimators, we define the errors and residuals in the following way: \\[ \\varepsilon_i=Y_i-{\\boldsymbol{X}}_i{\\boldsymbol{\\beta}}, \\mbox{ and } e_i=Y_i-\\widehat{Y}_i. \\] Theorem 5.2 (Least Squares Error) If  \\(\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}\\) is non-singular, then 1. \\(\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^Te_i={\\boldsymbol{0}}\\) 2. \\(\\sum\\limits_{i=1}^ne_i=0\\) Proof. \\[ \\begin{aligned} \\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^Te_i &amp;=\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T(Y_i-\\widehat{Y}_i) \\\\ &amp;=\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^TY_i-\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T\\widehat{Y}_i \\\\ &amp;=\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^TY_i-\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i{\\boldsymbol{\\widehat{\\beta}}} \\\\ &amp;=\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{X}}Y}-\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}{\\boldsymbol{\\widehat{\\beta}}} \\\\ &amp;=\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{X}}Y}-\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}} \\left( \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1} \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{X}}Y} \\right) \\\\ &amp;={\\boldsymbol{0}} \\end{aligned} \\] From the first row of (1) we get \\[ \\sum\\limits_{i=1}^n e_i=0. \\] Hence the result. "],["matrix-notation.html", "5.3 Matrix Notation", " 5.3 Matrix Notation Taking the definition of errors from the last section, we can write down a system of \\(n\\) linear equations: \\[ \\begin{aligned} Y_1 &amp;= {\\boldsymbol{X_1}}{\\boldsymbol{\\beta}} + \\varepsilon_1 \\\\ Y_2 &amp;= {\\boldsymbol{X_2}}{\\boldsymbol{\\beta}} + \\varepsilon_2 \\\\ &amp; \\vdots \\\\ Y_n &amp;= {\\boldsymbol{X_1}}{\\boldsymbol{\\beta}} + \\varepsilon_n \\end{aligned} \\] Define \\[ {\\boldsymbol{Y}}=\\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}_{n\\times1},\\ \\mathbb{X}=\\begin{bmatrix} 1 &amp; X_{[1]1} &amp; X_{[2]1} &amp; \\ldots &amp; X_{[k]1} \\\\ 1 &amp; X_{[1]2} &amp; X_{[2]2} &amp; \\ldots &amp; X_{[k]2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{[1]n} &amp; X_{[k]n} &amp; \\ldots &amp; X_{[k]n} \\end{bmatrix},\\mbox{ and } {\\boldsymbol{\\varepsilon}}=\\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}_{n\\times1}. \\] We can now rewrite the system as the following: \\[ {\\boldsymbol{Y}}={\\mathbb{X}}{\\boldsymbol{\\beta}}+{\\boldsymbol{\\varepsilon}}. \\] We also note that \\[ \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}=\\sum\\limits_{i=1}^n{\\boldsymbol{X}}^T_i{\\boldsymbol{X}}_i= {\\mathbb{X}}^T{\\mathbb{X}}, \\] and \\[ \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{X}}Y}=\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^TY_i= {\\mathbb{X}}^T{\\boldsymbol{Y}}. \\] So, we have write the least squares estimator as \\[ \\widehat{{\\boldsymbol{\\beta}}}=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{Y}}. \\] Similarly, the residual vector is \\[ {\\boldsymbol{e}}={\\boldsymbol{Y}}-{\\mathbb{X}}\\widehat{{\\boldsymbol{\\beta}}}. \\] As a consequence of 5.2, we can write \\[ {\\mathbb{X}}^T{\\boldsymbol{e}}={\\boldsymbol{0}}. \\] "],["linear-conditional-expectation-function.html", "Chapter 6 Linear Conditional Expectation Function ", " Chapter 6 Linear Conditional Expectation Function "],["variance-of-error.html", "6.1 Variance of Error", " 6.1 Variance of Error We first compute the (unconditional) variance of the error vector \\(\\pmb{e}\\). The covariance matrix \\[ \\mathbb{V}[\\pmb{e}]={\\mathbb{E}\\left[ \\pmb{e}\\pmb{e}&#39; \\right]}-{\\mathbb{E}\\left[ \\pmb{e} \\right]}{\\mathbb{E}\\left[ \\pmb{e}&#39; \\right]}={\\mathbb{E}\\left[ \\pmb{e}\\pmb{e}&#39; \\right]}\\stackrel{\\text{def}}{=}\\mathbb{D}. \\] For \\(i\\neq j\\), the errors \\(e_i\\),\\(e_j\\) are independent. As a result, \\({\\mathbb{E}\\left[ e_ie_j \\right]}={\\mathbb{E}\\left[ e_i \\right]}{\\mathbb{E}\\left[ e_j \\right]}=0\\). So, \\(\\mathbb{D}\\) is a diagonal matrix with the \\(i\\)-th diagonal element \\(\\sigma_i^2\\): \\[ \\mathbb{D}=\\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\sigma_n^2 \\end{bmatrix}. \\] "],["variance-of-ols-estimators.html", "6.2 Variance of OLS Estimators", " 6.2 Variance of OLS Estimators "],["large-sample-regression.html", "Chapter 7 Large-Sample Regression", " Chapter 7 Large-Sample Regression We assume that the best linear predictor, \\(\\mathscr{P}[Y|{\\boldsymbol{X}}]\\), of \\(Y\\) given \\({\\boldsymbol{X}}\\) is \\({\\boldsymbol{X}}{\\boldsymbol{\\beta}}\\). If we write \\[ Y={\\boldsymbol{X}}{\\boldsymbol{\\beta}}+e. \\] we have from Theorem 4.4 \\[{\\mathbb{E}\\left[ e \\right]}=0,\\mbox{ and }{\\mathbb{E}\\left[ {\\boldsymbol{X}}e \\right]}={\\boldsymbol{0}}.\\] We also assume that the dataset \\(\\{(Y_i,{\\boldsymbol{X}}_i)\\}\\) are taken i.i.d. from the joint distribution of \\((Y,{\\boldsymbol{X}})\\). For each \\(i\\), we can write \\[ Y_i={\\boldsymbol{X_i}}{\\boldsymbol{\\beta}}+e_i. \\] In matrix notation, we can write \\[ {\\boldsymbol{Y}}={\\mathbb{X}}{\\boldsymbol{\\beta}}+{\\boldsymbol{e}}. \\] Then \\[{\\mathbb{E}\\left[ {\\boldsymbol{e}} \\right]}={\\boldsymbol{0}}.\\] "],["consistency-of-ols-estimators.html", "7.1 Consistency of OLS Estimators", " 7.1 Consistency of OLS Estimators "],["asymptotic-normality.html", "7.2 Asymptotic Normality", " 7.2 Asymptotic Normality We start by revealing an alternative expression for the OLS estimators \\(\\widehat{{\\boldsymbol{\\beta}}}\\) using matrix notation. \\[ \\begin{aligned} \\widehat{{\\boldsymbol{\\beta}}} &amp;=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{Y}} \\\\ &amp;=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T({\\mathbb{X}}{\\boldsymbol{\\beta}}+{\\boldsymbol{e}}) \\\\ &amp;=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}({\\mathbb{X}}^T{\\mathbb{X}}){\\boldsymbol{\\beta}}+ \\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{e}} \\\\ &amp;={\\boldsymbol{\\beta}} + \\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{e}} \\end{aligned} \\] So, \\[\\begin{equation} \\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}} = \\left[\\mathbb{X}^T\\mathbb{X}\\right]^{-1}\\mathbb{X}^T{\\boldsymbol{e}} \\tag{7.1} \\end{equation}\\] We can then multiply by \\(\\sqrt{n}\\) both sides of Equation (7.1) to get \\[ \\begin{aligned} \\sqrt{n}\\left(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}}\\right) &amp;=\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i \\right)^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^Te_i \\right) \\\\ &amp;=\\widehat{\\mathbb Q}_{{\\boldsymbol{XX}}}^{-1} \\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^Te_i \\right) \\end{aligned} \\] From the consistency of OLS estimators, we already have \\[ \\widehat{\\mathbb Q}_{{\\boldsymbol{XX}}}\\xrightarrow[p]{\\quad\\quad}\\mathbb{Q}_{{\\boldsymbol{XX}}}\\] Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression. We first note (from i.i.d. and Theorem 4.4) that \\[ {\\mathbb{E}\\left[ {\\boldsymbol{X}}_ie_i \\right]}={\\mathbb{E}\\left[ {\\boldsymbol{X}}e \\right]}={\\boldsymbol{0}}. \\] Let us compute the covariance matrix of \\({\\boldsymbol{X}}_ie_i\\). Since the expectation vector is zero, we have \\[ \\mathbb{V}[{\\boldsymbol{X}}_ie_i]={\\mathbb{E}\\left[ {\\boldsymbol{X}}_ie_i({\\boldsymbol{X}}_ie_i)&#39; \\right]}={\\mathbb{E}\\left[ {\\boldsymbol{X}}e({\\boldsymbol{X}}e)&#39; \\right]}={\\mathbb{E}\\left[ {\\boldsymbol{X}}{\\boldsymbol{X}}&#39;e^2 \\right]}\\stackrel{\\text{def}}{=}\\mathbb{A}. \\] As any function of \\(\\{(Y_i,{\\boldsymbol{X}}_i)\\}\\)’s are independent, \\(\\{{\\boldsymbol{X}}_ie_i\\}\\)’s are independent. By the (multivariate) Central Limit Theorem, as \\(n\\to\\infty\\) \\[ \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_ie_i \\xrightarrow[d]{\\quad\\quad}\\mathcal{N}({\\boldsymbol{0}},\\mathbb{A}). \\] There is a small technicality here, we must have \\(\\mathbb{A}&lt;\\infty\\). This can be imposed by a stronger regularity condition on the moments, e.g., \\({\\mathbb{E}\\left[ Y^4 \\right]},{\\mathbb{E}\\left[ ||{\\boldsymbol{X}}||^4 \\right]}&lt;\\infty\\). Putting everything together, we conclude \\[ \\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}})\\xrightarrow[d]{\\quad\\quad} \\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\mathcal{N}({\\boldsymbol{0}},\\mathbb{A}) =\\mathcal{N}\\left(0,\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\mathbb{A}\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\right) \\] Theorem 7.1 (Asymptotic Distribution of OLS Estimators) We assume the following: 1. The observations \\(\\{(Y_i,{\\boldsymbol{X}}_i)\\}_{i=1}^n\\) are i.i.d from the joint distribution of \\((Y,{\\boldsymbol{X}})\\) 2. \\({\\mathbb{E}\\left[ Y^4 \\right]}&lt;\\infty\\) 3. \\({\\mathbb{E}\\left[ ||{\\boldsymbol{X}}||^4 \\right]}&lt;\\infty\\) 4. \\(\\mathbb{Q}_{{\\boldsymbol{XX}}}={\\mathbb{E}\\left[ {\\boldsymbol{X}}{\\boldsymbol{X}}&#39; \\right]}\\) is positive-definite. Under these assumptions, as \\(n\\to\\infty\\) \\[ \\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}})\\xrightarrow[d]{\\quad\\quad} \\mathcal{N}\\left({\\boldsymbol{0}},\\mathbb{V}_{{\\boldsymbol{\\beta}}}\\right), \\] where \\[\\mathbb{V}_{{\\boldsymbol{\\beta}}}\\stackrel{\\text{def}}{=}\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\mathbb{A}\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\] and \\(\\mathbb{Q}_{{\\boldsymbol{XX}}}={\\mathbb{E}\\left[ {\\boldsymbol{X}}{\\boldsymbol{X}}&#39; \\right]}\\), \\(\\mathbb{A}={\\mathbb{E}\\left[ {\\boldsymbol{X}}{\\boldsymbol{X}}&#39;e^2 \\right]}\\). The covariance matrix \\(\\mathbb{V}_{{\\boldsymbol{\\beta}}}\\) is called the asymptotic variance matrix of \\(\\widehat{{\\boldsymbol{\\beta}}}\\). The matrix is sometimes referred to as the sandwich form. "],["covariance-matrix-estimation.html", "7.3 Covariance Matrix Estimation", " 7.3 Covariance Matrix Estimation We now turn our attention to the estimation of the sandwich matrix using a finite sample. 7.3.1 Heteroskedastic Variance Theorem 7.1 presented the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}})\\) is \\[\\mathbb{V}_{{\\boldsymbol{\\beta}}} =\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}\\mathbb{A}\\mathbb{Q}_{{\\boldsymbol{XX}}}^{-1}.\\] Without imposing any homoskedasticity condition, we estimate \\(\\mathbb{V}_{{\\boldsymbol{\\beta}}}\\) using a plug-in estimator. We have already seen that \\(\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}=\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i{\\boldsymbol{X}}_i&#39;\\) is a natural estimator for \\(\\mathbb{Q}_{{\\boldsymbol{XX}}}\\). For \\(\\mathbb{A}\\), we use the moment estimator \\[ \\widehat{\\mathbb{A}}=\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i{\\boldsymbol{X}}_i&#39;\\widehat{e}_i^2, \\] where \\(\\widehat{e}_i=(Y_i-{\\boldsymbol{X}}_i&#39;\\widehat{{\\boldsymbol{\\beta}}})\\) is the \\(i\\)-th residual. As it turns out, \\(\\widehat{\\mathbb{A}}\\) is a consistent estimator for \\(\\mathbb{A}\\). As a result, we get the following plug-in estimator for \\(\\mathbb{V}_{{\\boldsymbol{\\beta}}}\\): \\[ \\widehat{\\mathbb{V}}_{{\\boldsymbol{\\beta}}}^{\\text{HC0}}= \\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\widehat{\\mathbb{A}}\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1} \\] The estimator is also consistent. For a proof, see Hensen 2013. As a consequence, we can get the following estimator for the variance, \\(\\mathbb{V}_{\\widehat{{\\boldsymbol{\\beta}}}}\\), of \\(\\widehat{{\\boldsymbol{\\beta}}}\\) in the heteroskedastic case. \\[\\begin{align} \\widehat{\\mathbb{V}}^{\\text{HC0}}_{\\widehat{{\\boldsymbol{\\beta}}}} &amp;=\\frac{1}{n}\\widehat{\\mathbb{V}}_{{\\boldsymbol{\\beta}}}^{\\text{HC0}} \\\\ &amp;=\\frac{1}{n}\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\widehat{\\mathbb{A}}\\widehat{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1} \\\\ &amp;=\\frac{1}{n}\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i{\\boldsymbol{X}}_i&#39;\\right)^{-1} \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i{\\boldsymbol{X}}_i&#39;\\widehat{e}_i^2\\right) \\left(\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i{\\boldsymbol{X}}_i&#39;\\right)^{-1} \\\\ &amp;=\\left(\\mathbb{X}\\mathbb{X}&#39;\\right)^{-1} \\mathbb{X}\\mathbb{D}\\mathbb{X}&#39; \\left(\\mathbb{X}\\mathbb{X}&#39;\\right)^{-1} \\end{align}\\] where \\(\\mathbb{D}\\) is an \\(n\\times n\\) diagonal matrix with diagonal entries \\(\\widehat{e}_1^2,\\widehat{e}_2^2,\\ldots,\\widehat{e}_n^2\\). The estimator \\(\\widehat{\\mathbb{V}}^{\\text{HC0}}_{\\widehat{{\\boldsymbol{\\beta}}}}\\) is referred to as the robust error variance estimator for the OLS coefficients \\(\\widehat{{\\boldsymbol{\\beta}}}\\). 7.3.2 Homeskedastic Variance "],["references.html", "References", " References "],["matrix-algebra.html", "A Matrix Algebra", " A Matrix Algebra In this book, we reserve boldface letter to denote vectors (of scalars and random variables), and “blackboard bold” typeface to denote matrices. We always write a vector as a column \\[ \\pmb{v}=\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_k \\end{bmatrix}_{k\\times1} \\] Definition A.1 (Transpose of a Matrix) Let \\(\\mathbb{A}_{k\\times l}\\) be a matrix, it’s transpose, denoted \\(\\mathbb{A}^T\\), is an \\(l\\times k\\) matrix such that the \\((i,j)\\)-th entry of \\(\\mathbb{A}\\) becomes the \\((j,i)\\)-th entry of \\(\\mathbb{A}^T\\). Definition A.2 (Sum of Matrices) Let \\(\\mathbb{A},\\mathbb{B}\\) are matrices both of size \\(k\\times l\\), then the sum \\(\\mathbb{A}+\\mathbb{B}\\) is defined as the another matrix \\(\\mathbb{C}\\) size \\(k\\times l\\) such that the \\((i,j)\\)-th entry is the sum of the \\((i,j)\\)-th entries of \\(\\mathbb A\\) and \\(\\mathbb B\\). \\[ \\mathbb{C}=\\begin{bmatrix} a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; \\ldots &amp; a_{1l}+b_{1l} \\\\ a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; \\ldots &amp; a_{2l}+b_{2l} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{k1}+b_{k1} &amp; a_{k2}+b_{k2} &amp; \\ldots &amp; a_{kl}+b_{kl} \\\\ \\end{bmatrix}_{k\\times l} \\] Definition A.3 (Product of Matrices) Let \\(\\mathbb{A},\\mathbb{B}\\) are matrices both of size \\(k\\times l\\), then the sum \\(\\mathbb{A}+\\mathbb{B}\\) is defined as the another matrix \\(\\mathbb{C}\\) size \\(k\\times l\\) such that the \\((i,j)\\)-th entry is the sum of the \\((i,j)\\)-th entries of \\(\\mathbb A\\) and \\(\\mathbb B\\). \\[ \\mathbb{C}=\\begin{bmatrix} a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; \\ldots &amp; a_{1l}+b_{1l} \\\\ a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; \\ldots &amp; a_{2l}+b_{2l} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{k1}+b_{k1} &amp; a_{k2}+b_{k2} &amp; \\ldots &amp; a_{kl}+b_{kl} \\\\ \\end{bmatrix}_{k\\times l} \\] "],["matrix-calculus.html", "B Matrix Calculus", " B Matrix Calculus "]]
